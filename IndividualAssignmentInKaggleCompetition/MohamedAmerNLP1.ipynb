{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "1. Import Section\n",
    "2. EDA\n",
    "3. Baseline Model\n",
    "4. Model Enhancements\n",
    "    <br>4.1 dealing with abbreviation issue:\n",
    "    <br>4.2 dealing with contractions issue:\n",
    "    <br>4.3 dealing with URL's/HTML tags issue:\n",
    "    <br>4.4 dealing with Emojis issue:\n",
    "    <br>4.5 dealing with numbers issue:\n",
    "    <br>4.6 dealing with punctuation issue:\n",
    "    <br>4.7 removing stopwords:\n",
    "    <br>4.8 lemmatizing/stemming:\n",
    "5. Result\n",
    "6. Other tasks\n",
    "7. Future Enhancements\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install inflect\n",
    "#!pip install num2word\n",
    "#!pip install num2words\n",
    "#!pip install contractions\n",
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Mo import moFunctions\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import re,csv\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import inflect, num2word,num2words,contractions, emoji \n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "testDF = pd.read_csv('test.csv')\n",
    "submitDF = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 1145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 1147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submitDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword       61\n",
       "location    2533\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moFunctions.checkNulls(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword       26\n",
       "location    1105\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moFunctions.checkNulls(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 1151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moFunctions.checkNulls(submitDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "body%20bags              41\n",
       "harm                     41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                     104\n",
       "New York                 71\n",
       "United States            50\n",
       "London                   45\n",
       "Canada                   29\n",
       "                       ... \n",
       "Tama, Iowa                1\n",
       "Lake Highlands            1\n",
       "Minneapolis/St. Paul      1\n",
       "Metro Manila              1\n",
       "Halfrica                  1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\""
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[\"text\"].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword    location                                               text  \\\n",
       "31  48  ablaze  Birmingham  @bbcmtd Wholesale Markets ablaze http://t.co/l...   \n",
       "\n",
       "    target  \n",
       "31       1  "
      ]
     },
     "execution_count": 1155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[trainDF['id'] == 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF['keyword'] = testDF['keyword'].replace(np.nan, '')\n",
    "trainDF['keyword'] = trainDF['keyword'].replace(np.nan, '')\n",
    "testDF['location'] = testDF['location'].replace(np.nan, '')\n",
    "trainDF['location'] = trainDF['location'].replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model\n",
    "Since we are going to predict that this tweet is disaster or not then we need first to create the sparse matrix that is needed for the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use a CountVectorizer as a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer()\n",
    "trainSparseMat = countVec.fit_transform(trainDF[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x21637 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 111497 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSparseMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainSparseMat[0:1].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily we create the testSparseMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSparseMat = countVec.transform(testDF[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21637)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainSparseMat.todense().shape)\n",
    "print(trainSparseMat.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 21637)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(testSparseMat.todense().shape)\n",
    "print(testSparseMat.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then using logistic regression i'm going to use it as my base model that i'll try to enhance afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets first form our train/test X&y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.78      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.63      3263\n",
      "   macro avg       0.50      0.32      0.39      3263\n",
      "weighted avg       1.00      0.63      0.78      3263\n",
      "\n",
      "Average f1_score: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "countVec = CountVectorizer()\n",
    "trainSparseMat = countVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = countVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "ytest = submitDF['target']\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "y_pred = logreg.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then our base accuracy is **63%** and f1 score is **0.78** and let's see how we can improve this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Enhancements \n",
    "**(reload dataframes again ie. run step 1 then 3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Appreviations issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "\n",
    "        # File Access mode [Read Mode]\n",
    "        with open(fileName, \"r\") as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9]+', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    return ' '.join(user_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['text'] = trainDF['text'].apply(lambda x:  translator(x)  ) \n",
    "testDF['text'] = testDF['text'].apply(lambda x:  translator(x)  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    #emojies to text\n",
    "    #text = emoji.demojize(text)\n",
    "    #text = ' '.join(word_tokenize(text))\n",
    "    # fixing contraction issue ie. i'm, don't,..\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # removingURL's\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # removing mentioned\n",
    "    #text = re.sub(\"@[A-Za-z0-9]+\",\"\",text)\n",
    "    # removingHtmlTags\n",
    "    html=re.compile(r'<.*?>')\n",
    "    # removingweird characters\n",
    "    text = html.sub(r'',text)\n",
    "    text = ''.join([x for x in text if x in string.printable])\n",
    "    # removingEmojies\n",
    "#     emojies = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001F64F\"\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     text = emojies.sub(r'', text)\n",
    "    # remove numbers\n",
    "    # text = \"\".join([i for i in text if not i.isdigit()])\n",
    "    # trying to replace numbers\n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        if word.isdigit():\n",
    "            new_word = num2words.num2words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    text = new_words\n",
    "     # remove punctuation\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    # remove stopwords\n",
    "    #text = \"\".join([i for i in text if i not in stopwords.words('english')])\n",
    "    #text_tokens = word_tokenize(text)\n",
    "    #text = (\" \").join([word for word in text_tokens if not word in stopwords.words()])\n",
    "    # Lemmatize\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #text = \"\".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    porter = PorterStemmer()\n",
    "    text = \"\".join([porter.stem(i) for i in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['text'] = trainDF['text'].map( lambda text : cleanText(text))\n",
    "testDF['text'] = testDF['text'].map( lambda text : cleanText(text))\n",
    "#trainDF['text'] = trainDF['text'].map(lambda x : ' '.join(word_tokenize(x)))\n",
    "#testDF['text'] = testDF['text'].map(lambda x : ' '.join(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to see lowercasing and \\n replacement\n",
    "trainDF['text']=trainDF['text'].str.lower()\n",
    "testDF['text']=testDF['text'].str.lower()\n",
    "trainDF['text']=trainDF['text'].str.replace(\"\\n\",\" \")\n",
    "testDF['text']=testDF['text'].str.replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF[trainDF['id'] == 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in testDF.iterrows():\n",
    "#     print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67      3263\n",
      "   macro avg       0.50      0.33      0.40      3263\n",
      "weighted avg       1.00      0.67      0.80      3263\n",
      "\n",
      "Average f1_score: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# best 0.80879\n",
    "\n",
    "countVec = CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "trainSparseMat = countVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = countVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "ytest = submitDF['target']\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "y_pred = logreg.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitDF['target'] = predictions\n",
    "submitDF.to_csv('my_submission24.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best f1-score  so far (**0.80879**).\n",
    "<img src=\"bestscore3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Trying other ML/Vectorizer\n",
    "\n",
    "\n",
    "# words = countVec.get_feature_names()\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_emojis(text):\n",
    "#     for emot in UNICODE_EMO:\n",
    "#         text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = convert_emojis(testDF['text'][80] ) \n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check if we can unify the countries as much as we can\n",
    "\n",
    "unify_country = { '  Glasgow ' : 'GBR'\n",
    ", '  Melbourne, Australia' : 'AUS'\n",
    ", ' 616 ¬â√õ¬¢ Kentwood , MI ' : 'USA'\n",
    ", ' Alberta' : 'CAN'\n",
    ", ' Eugene, Oregon' : 'USA'\n",
    ", ' Indiana' : 'USA'\n",
    ", ' Little Rock, AR' : 'USA'\n",
    ", ' Miami Beach' : 'USA'\n",
    ", ' Nevada Carson City,Freeman St' : 'USA'\n",
    ", ' Neverland ' : 'USA'\n",
    ", ' New Delhi ' : 'IND'\n",
    ", ' New England' : 'UK'\n",
    ", ' Nxgerxa' : 'USA'\n",
    ", ' Quantico Marine Base, VA.' : 'USA'\n",
    ", ' Queensland, Australia' : 'AUS'\n",
    ", ' Tropical SE FLorida' : 'USA'\n",
    ", '? miranda ? 521 mi' : 'USA'\n",
    ", '???????, Texas' : 'USA'\n",
    ", '?????????????, Thailand ' : 'THA'\n",
    ", '\\'Merica' : 'USA'\n",
    ", '\\'SAN ANTONIOOOOO\\'' : 'USA'\n",
    ", '(Spain)' : 'ESP'\n",
    ", '#EngleWood CHICAGO ' : 'USA'\n",
    ", '#FLIGHTCITY UK  ' : 'UK'\n",
    ", '#goingdownthetoilet Illinois' : 'USA'\n",
    ", '#iminchina' : 'CHN'\n",
    ", '#NewcastleuponTyne #UK' : 'UK'\n",
    ", '#SOUTHAMPTON ENGLAND' : 'UK'\n",
    ", '#WashingtonState #Seattle' : 'USA'\n",
    ", '|| c h i c a g o ||' : 'USA'\n",
    ", '√•_√•_Los Mina City¬â√£¬¢' : 'USA'\n",
    ", '√•¬°√•¬°Midwest ¬â√õ¬¢¬â√õ¬¢' : 'USA'\n",
    ", '√å√∏√•√Ä√•_T: 40.736324,-73.990062' : 'USA'\n",
    ", '√å√èT: -26.695807,27.837865' : 'ZAF'\n",
    ", '√å√èT: 1.50225,103.742992' : 'MYS'\n",
    ", '√å√èT: 10.614817868480726,12.195582811791382' : 'NGA'\n",
    ", '√å√èT: 19.123127,72.825133' : 'IND'\n",
    ", '√å√èT: 27.9136024,-81.6078532' : 'USA'\n",
    ", '√å√èT: 30.307558,-81.403118' : 'USA'\n",
    ", '√å√èT: 33.209923,-87.545328' : 'USA'\n",
    ", '√å√èT: 35.223347,-80.827834' : 'USA'\n",
    ", '√å√èT: 36.142163,-95.979189' : 'USA'\n",
    ", '√å√èT: 39.982988,-75.261624' : 'USA'\n",
    ", '√å√èT: 40.562796,-75.488849' : 'USA'\n",
    ", '√å√èT: 40.707762,-74.014213' : 'USA'\n",
    ", '√å√èT: 41.252426,-96.072013' : 'USA'\n",
    ", '√å√èT: 42.910975,-78.865828' : 'USA'\n",
    ", '√å√èT: 43.631838,-79.55807' : 'CAN'\n",
    ", '√å√èT: 6.4682,3.18287' : 'NGA'\n",
    ", '√å√èT: 6.488400524109015,3.352798039832285' : 'NGA'\n",
    ", '11th dimension, los angeles' : 'USA'\n",
    ", '1313 W.Patrick St, Frederick' : 'USA'\n",
    ", '140920-21 & 150718-19 BEIJING' : 'CHN'\n",
    ", '1648 Queen St. West, Toronto.' : 'CAN'\n",
    ", '19.600858, -99.047821' : 'MEX'\n",
    ", '21.462446,-158.022017' : 'USA'\n",
    ", '261 5th Avenue New York, NY ' : 'USA'\n",
    ", '2B Hindhede Rd, Singapore' : 'SGP'\n",
    ", '412 NW 5th Ave. Portland OR' : 'USA'\n",
    ", '48.870833,2.399227' : 'FRA'\n",
    ", '50% Queanbeyan - 50% Sydney' : 'AUS'\n",
    ", '518 √•√° NY' : 'USA'\n",
    ", '52.479722, 62.184971' : 'KAZ'\n",
    ", '570 Vanderbilt; Brooklyn, NY' : 'USA'\n",
    ", '828/704(Soufside)/while looking goofy in NJ' : 'USA'\n",
    ", 'Ab, Canada' : 'CAN'\n",
    ", 'Aberdeenshire' : 'UK'\n",
    ", 'Absecon, NJ' : 'USA'\n",
    ", 'Abuja' : 'NGA'\n",
    ", 'Abuja, Nigeria' : 'NGA'\n",
    ", 'Abuja,Nigeria' : 'NGA'\n",
    ", 'ACCRA GHANA' : 'GHA'\n",
    ", 'Accra,Ghana' : 'GHA'\n",
    ", 'Adelaide, Australia' : 'AUS'\n",
    ", 'Adelaide, South Australia' : 'AUS'\n",
    ", 'Afghanistan' : 'AFG'\n",
    ", 'Afghanistan, USA' : 'USA'\n",
    ", 'Aix-en-Provence, France' : 'FRA'\n",
    ", 'AKRON OHIO USA' : 'USA'\n",
    ", 'Alabama' : 'USA'\n",
    ", 'Alabama, USA' : 'USA'\n",
    ", 'Alameda and Pleasanton, CA' : 'USA'\n",
    ", 'Alameda, CA' : 'USA'\n",
    ", 'Alaska' : 'USA'\n",
    ", 'Alaska, USA' : 'USA'\n",
    ", 'Albany/NY' : 'USA'\n",
    ", 'Alberta ' : 'CAN'\n",
    ", 'Alberta | Sask. | Montana' : 'CAN'\n",
    ", 'Alberta Pack' : 'CAN'\n",
    ", 'alberta, canada' : 'CAN'\n",
    ", 'Alberta, VA' : 'CAN'\n",
    ", 'Albuquerque New Mexico' : 'MEX'\n",
    ", 'Alexandria, Egypt.' : 'EGY'\n",
    ", 'Alexandria, VA' : 'USA'\n",
    ", 'Alexandria, VA, USA' : 'USA'\n",
    ", 'Alger-New York-San Francisco' : 'USA'\n",
    ", 'Alicante, Spain' : 'ESP'\n",
    ", 'Alicante, Valencia' : 'ESP'\n",
    ", 'Alliston Ontario' : 'CAN'\n",
    ", 'Alphen aan den Rijn, Holland' : 'NLD'\n",
    ", 'Alvin, TX' : 'USA'\n",
    ", 'Amazon Seller , Propagandist' : 'USA'\n",
    ", 'America' : 'USA'\n",
    ", 'America | New Zealand ' : 'USA'\n",
    ", 'America of Founding Fathers' : 'USA'\n",
    ", 'Americas Newsroom' : 'USA'\n",
    ", 'Ames, IA' : 'USA'\n",
    ", 'Ames, Iowa' : 'USA'\n",
    ", 'Amman, Jordan' : 'JOR'\n",
    ", 'Amman,Jordan' : 'JOR'\n",
    ", 'Amsterdam' : 'NLD'\n",
    ", 'amsterdayum 120615 062415' : 'NLD'\n",
    ", 'Anaheim' : 'USA'\n",
    ", 'Anchorage, AK' : 'USA'\n",
    ", 'Anderson, SC' : 'USA'\n",
    ", 'Ankara - Malatya - ad Orontem' : 'TUR'\n",
    ", 'Anna Maria, FL' : 'USA'\n",
    ", 'Annapolis, MD' : 'USA'\n",
    ", 'Antigua ?? NYC ' : 'USA'\n",
    ", 'Antioch, CA ' : 'USA'\n",
    ", 'antioch, california' : 'USA'\n",
    ", 'anzio,italy' : 'ITA'\n",
    ", 'ARGENTINA' : 'ARG'\n",
    ", 'ARIZONA' : 'USA'\n",
    ", 'Arizona ' : 'USA'\n",
    ", 'Arkansas' : 'USA'\n",
    ", 'Arkansas, Jonesboro' : 'USA'\n",
    ", 'Arlington, TX' : 'USA'\n",
    ", 'Arlington, VA' : 'USA'\n",
    ", 'Arlington, VA and DC' : 'USA'\n",
    ", 'Arnhem, the Netherlands' : 'NLD'\n",
    ", 'Arthas US' : 'USA'\n",
    ", 'Arundel ' : 'USA'\n",
    ", 'Arvada, CO' : 'USA'\n",
    ", 'Asgard' : 'USA'\n",
    ", 'Ashburn, VA' : 'USA'\n",
    ", 'Asheboro, NC' : 'USA'\n",
    ", 'Asheville, NC' : 'USA'\n",
    ", 'Ashford, Kent, United Kingdom' : 'UK'\n",
    ", 'Ashland, Oregon' : 'USA'\n",
    ", 'Asunci√å_n-PY / T√å_bingen-GER' : 'GER'\n",
    ", 'Athens - Nicosia' : 'GRC'\n",
    ", 'Athens, Greece' : 'GRC'\n",
    ", 'Athens,Greece' : 'GRC'\n",
    ", 'ATL ? SEA ' : 'USA'\n",
    ", 'ATL ??' : 'USA'\n",
    ", 'ATL, GA' : 'USA'\n",
    ", 'ATL??AL??' : 'USA'\n",
    ", 'Atlanta' : 'USA'\n",
    ", 'Atlanta - FAU class of \\'18' : 'USA'\n",
    ", 'ATLANTA , GEORGIA ' : 'USA'\n",
    ", 'Atlanta g.a.' : 'USA'\n",
    ", 'Atlanta Georgia' : 'USA'\n",
    ", 'Atlanta Georgia ' : 'USA'\n",
    ", 'Atlanta, GA' : 'USA'\n",
    ", 'Atlanta, Georgia' : 'USA'\n",
    ", 'Atlanta, Georgia USA' : 'USA'\n",
    ", 'Atlanta,Ga' : 'USA'\n",
    ", 'Atlanta(ish), GA' : 'USA'\n",
    ", 'Atlantic Highlands, NJ' : 'USA'\n",
    ", 'Atlantic, IA' : 'USA'\n",
    ", 'Auburn ' : 'USA'\n",
    ", 'Auburn, AL' : 'USA'\n",
    ", 'Auckland' : 'USA'\n",
    ", 'Auckland, New Zealand' : 'USA'\n",
    ", 'Augusta, GA' : 'USA'\n",
    ", 'Augusta, Maine, 04330' : 'USA'\n",
    ", 'Aurora, IL' : 'USA'\n",
    ", 'Aurora, Ontario ' : 'CAN'\n",
    ", 'AUS' : 'AUS'\n",
    ", 'Austin' : 'USA'\n",
    ", 'Austin | San Diego' : 'USA'\n",
    ", 'Austin TX' : 'USA'\n",
    ", 'Austin, Texas' : 'USA'\n",
    ", 'Austin, TX' : 'USA'\n",
    ", 'Austin/Los Angeles' : 'USA'\n",
    ", 'Australia' : 'AUS'\n",
    ", 'Australia ' : 'AUS'\n",
    ", 'AUSTRALIA-SOUTHAFRICA-CAMBODIA' : 'AUS'\n",
    ", 'Australian Capital Territory' : 'AUS'\n",
    ", 'Aveiro, Portugal' : 'PRT'\n",
    ", 'Avon' : 'USA'\n",
    ", 'Avon, OH' : 'USA'\n",
    ", 'AZ' : 'USA'\n",
    ", 'Back East in PA' : 'USA'\n",
    ", 'Bahrain' : 'BHR'\n",
    ", 'Baker City Oregon' : 'USA'\n",
    ", 'Bakersfield, CA' : 'USA'\n",
    ", 'Bakersfield, California' : 'USA'\n",
    ", 'Balikesir - Eskisehir' : 'USA'\n",
    ", 'Baltimore' : 'USA'\n",
    ", 'Baltimore, MD' : 'USA'\n",
    ", 'Bandar Lampung, Indonesia' : 'IDN'\n",
    ", 'Bandung' : 'IDN'\n",
    ", 'bangalore' : 'IND'\n",
    ", 'Bangalore City, India' : 'IND'\n",
    ", 'Bangalore, India' : 'IND'\n",
    ", 'Bangalore. India' : 'IND'\n",
    ", 'Bangkok' : 'THA'\n",
    ", 'Bangkok Thailand' : 'THA'\n",
    ", 'Bangor, Co.Down' : 'UK'\n",
    ", 'Barcelona, Spain' : 'ESP'\n",
    ", 'Bartholomew County, Indiana' : 'USA'\n",
    ", 'Based in CA - Serve Nationwide' : 'USA'\n",
    ", 'Based out of Portland, Oregon' : 'USA'\n",
    ", 'Basketball City, USA ' : 'USA'\n",
    ", 'Basking Ridge, NJ' : 'USA'\n",
    ", 'Bathtub de Bett ' : 'USA'\n",
    ", 'Baton Rouge' : 'USA'\n",
    ", 'Baton Rouge, LA' : 'USA'\n",
    ", 'Bay Area' : 'USA'\n",
    ", 'Bay Area, CA' : 'USA'\n",
    ", 'Baydestrian' : 'USA'\n",
    ", 'Bayonne, NJ' : 'USA'\n",
    ", 'BC' : 'USA'\n",
    ", 'Beacon Hills' : 'USA'\n",
    ", 'beacon hills ' : 'USA'\n",
    ", 'Beaumont, TX' : 'USA'\n",
    ", 'Beautiful British Columbia' : 'CAN'\n",
    ", 'Bedford IN ' : 'USA'\n",
    ", 'Bedford, England' : 'UK'\n",
    ", 'beijing .China' : 'CHN'\n",
    ", 'Beirut, Lebanon' : 'LBN'\n",
    ", 'Beirut/Toronto' : 'CAN'\n",
    ", 'Belbroughton, England' : 'UK'\n",
    ", 'Belfast' : 'UK'\n",
    ", 'Belgium' : 'BEL'\n",
    ", 'Belgrade' : 'SRB'\n",
    ", 'belleville' : 'USA'\n",
    ", 'Belleville, Illinois' : 'USA'\n",
    ", 'Bellevue NE' : 'USA'\n",
    ", 'Bellville, Ohio' : 'USA'\n",
    ", 'Bend, Oregon' : 'USA'\n",
    ", 'Benicia, CA ' : 'USA'\n",
    ", 'Benton City, Washington' : 'USA'\n",
    ", 'Berlin - Germany' : 'GER'\n",
    ", 'Berlin, Germany' : 'GER'\n",
    ", 'BIG D  HOUSTON/BOSTON/DENVER' : 'USA'\n",
    ", 'Biloxi, Mississippi' : 'USA'\n",
    ", 'Birdland, New Meridian, FD' : 'USA'\n",
    ", 'Birmingham' : 'UK'\n",
    ", 'Birmingham & Bristol' : 'UK'\n",
    ", 'Birmingham and the Marches' : 'UK'\n",
    ", 'Birmingham UK' : 'UK'\n",
    ", 'Birmingham, England' : 'UK'\n",
    ", 'Birmingham, UK' : 'UK'\n",
    ", 'Birmingham, United Kingdom' : 'UK'\n",
    ", 'Bishops Lydeard, England' : 'UK'\n",
    ", 'Bishops Stortford, England' : 'UK'\n",
    ", 'Black Canyon New River, AZ' : 'USA'\n",
    ", 'Blackpool' : 'UK'\n",
    ", 'Blackpool, England, UK.' : 'UK'\n",
    ", 'Bleak House' : 'UK'\n",
    ", 'Bloomington, IN' : 'USA'\n",
    ", 'Bloomington, Indiana' : 'USA'\n",
    ", 'Boise, Idaho' : 'USA'\n",
    ", 'Bolivar, MO' : 'USA'\n",
    ", 'Bolton & Tewkesbury, UK' : 'UK'\n",
    ", 'Bombardment Bay' : 'USA'\n",
    ", 'Bon Temps Louisiana' : 'USA'\n",
    ", 'Books Published, USA' : 'USA'\n",
    ", 'Born in Baltimore Living in PA' : 'USA'\n",
    ", 'Boston' : 'USA'\n",
    ", 'Boston ¬â√õ¬¢ Cape Cod ?' : 'USA'\n",
    ", 'Boston MA' : 'USA'\n",
    ", 'Boston, MA' : 'USA'\n",
    ", 'Boston, Massachusetts' : 'USA'\n",
    ", 'Boston/Montreal ' : 'USA'\n",
    ", 'Boulder' : 'USA'\n",
    ", 'Boulder, CO' : 'USA'\n",
    ", 'Bournemouth' : 'USA'\n",
    ", 'Bournemouth, Dorset, UK' : 'UK'\n",
    ", 'Bow, NH' : 'USA'\n",
    ", 'Bozeman, Montana' : 'USA'\n",
    ", 'Brackley Beach, PE, Canada' : 'CAN'\n",
    ", 'Brasil' : 'BRA'\n",
    ", 'Brasil, Fortaleza ce' : 'BRA'\n",
    ", 'Brasil,SP' : 'BRA'\n",
    ", 'Brazil' : 'BRA'\n",
    ", 'Brazil ' : 'BRA'\n",
    ", 'Brazos Valley, Texas' : 'USA'\n",
    ", 'Brecksville, OH' : 'USA'\n",
    ", 'Bremerton, WA' : 'USA'\n",
    ", 'Brentwood, NY' : 'USA'\n",
    ", 'Brentwood,TN' : 'USA'\n",
    ", 'Bridport, England' : 'UK'\n",
    ", 'Brighton and Hove' : 'UK'\n",
    ", 'Brisbane' : 'AUS'\n",
    ", 'Brisbane Australia' : 'AUS'\n",
    ", 'brisbane, australia' : 'AUS'\n",
    ", 'Brisbane, Queensland' : 'AUS'\n",
    ", 'Brisbane.' : 'AUS'\n",
    ", 'Bristol' : 'UK'\n",
    ", 'Bristol, England' : 'UK'\n",
    ", 'Bristol, UK' : 'UK'\n",
    ", 'British Columbia, Canada' : 'CAN'\n",
    ", 'British girl in Texas' : 'USA'\n",
    ", 'Bronx NY' : 'USA'\n",
    ", 'Bronx NYC / M-City NY' : 'USA'\n",
    ", 'Bronx, New York' : 'USA'\n",
    ", 'Bronx, NY' : 'USA'\n",
    ", 'Brooklyn' : 'USA'\n",
    ", 'Brooklyn, New York' : 'USA'\n",
    ", 'Brooklyn, NY' : 'USA'\n",
    ", 'BROOKLYN, NYC' : 'USA'\n",
    ", 'Broomfield, CO' : 'USA'\n",
    ", 'BrowardCounty // Florida ' : 'USA'\n",
    ", 'Bucks County, Pa' : 'USA'\n",
    ", 'Budapest, Hungary' : 'HUN'\n",
    ", 'Buenos Aires' : 'ARG'\n",
    ", 'buenos aires argentina' : 'ARG'\n",
    ", 'Buenos Aires, Argentina' : 'ARG'\n",
    ", 'Buffalo NY' : 'USA'\n",
    ", 'Buffalo, NY' : 'USA'\n",
    ", 'Burbank,CA' : 'USA'\n",
    ", 'Bushkill pa' : 'USA'\n",
    ", 'CA' : 'USA'\n",
    ", 'CA physically- Boston Strong?' : 'USA'\n",
    ", 'Cairo, Egypt' : 'EGY'\n",
    ", 'Cairo, Egypt.' : 'EGY'\n",
    ", 'Calgary' : 'CAN'\n",
    ", 'Calgary, AB' : 'CAN'\n",
    ", 'Calgary, AB, Canada' : 'CAN'\n",
    ", 'Calgary, Alberta' : 'CAN'\n",
    ", 'Calgary, Alberta, Canada' : 'CAN'\n",
    ", 'Calgary, Canada' : 'CAN'\n",
    ", 'calgary,ab' : 'CAN'\n",
    ", 'Calgary,AB, Canada' : 'CAN'\n",
    ", 'Calgary/Airdrie/RedDeer/AB' : 'CAN'\n",
    ", 'California' : 'USA'\n",
    ", 'California ' : 'USA'\n",
    ", 'california | oregon | peru |' : 'USA'\n",
    ", 'california mermaid ? ' : 'USA'\n",
    ", 'California or Colorado' : 'USA'\n",
    ", 'California, United States' : 'USA'\n",
    ", 'California, USA' : 'USA'\n",
    ", 'CAMARILLO, CA' : 'USA'\n",
    ", 'Cambridge, Massachusetts' : 'USA'\n",
    ", 'Cambridge, Massachusetts, U.S.' : 'USA'\n",
    ", 'Canada' : 'CAN'\n",
    ", 'Canada ' : 'CAN'\n",
    ", 'Canada BC' : 'CAN'\n",
    ", 'Canada Eh! ' : 'CAN'\n",
    ", 'Canberra, Australian Capital Territory' : 'AUS'\n",
    ", 'Cape Cod, Massachusetts USA' : 'USA'\n",
    ", 'Cape Town' : 'ZAF'\n",
    ", 'Cape Town, Khayelitsha' : 'ZAF'\n",
    ", 'Cardiff, UK' : 'UK'\n",
    ", 'Carol Stream, Illinois' : 'USA'\n",
    ", 'Caserta-Roma, Italy ' : 'ITA'\n",
    ", 'Cassadaga Florida' : 'USA'\n",
    ", 'Castaic, CA' : 'USA'\n",
    ", 'Catalonia, Spain' : 'ESP'\n",
    ", 'Central Coast, California' : 'USA'\n",
    ", 'Central Florida' : 'USA'\n",
    ", 'Central Illinois' : 'USA'\n",
    ", 'Chandler, AZ' : 'USA'\n",
    ", 'Chapel Hill, NC' : 'USA'\n",
    ", 'Chappaqua NY and Redlands CA' : 'USA'\n",
    ", 'Charleston, IL' : 'USA'\n",
    ", 'Charlotte' : 'USA'\n",
    ", 'Charlotte ' : 'USA'\n",
    ", 'Charlotte County Florida' : 'USA'\n",
    ", 'Charlotte NC' : 'USA'\n",
    ", 'Charlotte, N.C.' : 'USA'\n",
    ", 'Charlotte, NC' : 'USA'\n",
    ", 'Charlotte, NC | K√å¬¶ln, NRW' : 'USA'\n",
    ", 'Charlotte, North Carolina' : 'USA'\n",
    ", 'Charlottetown' : 'USA'\n",
    ", 'Chatham, IL' : 'USA'\n",
    ", 'Cherry Creek Denver CO' : 'USA'\n",
    ", 'Cheshire. London. #allover' : 'UK'\n",
    ", 'Chester, IL' : 'USA'\n",
    ", 'Chicago' : 'USA'\n",
    ", 'Chicago - Lake Buena Vista' : 'USA'\n",
    ", 'CHICAGO (312)' : 'USA'\n",
    ", 'Chicago Area' : 'USA'\n",
    ", 'Chicago Heights, IL' : 'USA'\n",
    ", 'Chicago IL' : 'USA'\n",
    ", 'Chicago, but Philly is home' : 'USA'\n",
    ", 'Chicago, IL' : 'USA'\n",
    ", 'Chicago, IL ' : 'USA'\n",
    ", 'Chicago, IL 60607' : 'USA'\n",
    ", 'Chicago, Illinois' : 'USA'\n",
    ", 'Chicago,Illinois' : 'USA'\n",
    ", 'Chicagoland' : 'USA'\n",
    ", 'ChicagoRObotz' : 'USA'\n",
    ", 'China' : 'CHN'\n",
    ", 'Chippenham/Bath, UK' : 'UK'\n",
    ", 'Chiswick, London' : 'UK'\n",
    ", 'Chorley, Lancashire, UK' : 'UK'\n",
    ", 'City of Angels, CA' : 'USA'\n",
    ", 'City of London, London' : 'UK'\n",
    ", 'Ciudad Aut√å_noma de Buenos Aires, Argentina' : 'ARG'\n",
    ", 'Clayton, NC' : 'USA'\n",
    ", 'Clearwater, FL' : 'USA'\n",
    ", 'Cleveland, OH' : 'USA'\n",
    ", 'Cleveland, OH - San Diego, CA' : 'USA'\n",
    ", 'Coasts of Maine & California' : 'USA'\n",
    ", 'Cochrane, Alberta, Canada' : 'CAN'\n",
    ", 'Coconut Creek, Florida' : 'USA'\n",
    ", 'Colchester Essex ' : 'UK'\n",
    ", 'College Station, TX' : 'USA'\n",
    ", 'Colombia' : 'USA'\n",
    ", 'Colombo,Sri Lanka.' : 'USA'\n",
    ", 'Colonial Heights, VA' : 'USA'\n",
    ", 'ColoRADo' : 'USA'\n",
    ", 'Colorado Springs' : 'USA'\n",
    ", 'Colorado, USA' : 'USA'\n",
    ", 'Columbia Heights, MN' : 'USA'\n",
    ", 'Columbia, SC' : 'USA'\n",
    ", 'Columbus' : 'USA'\n",
    ", 'Columbus ?? North Carolina' : 'USA'\n",
    ", 'columbus ohio' : 'USA'\n",
    ", 'Columbus, Georgia' : 'USA'\n",
    ", 'Columbus, OH' : 'USA'\n",
    ", 'Concord, CA' : 'USA'\n",
    ", 'Concord, N.C.' : 'USA'\n",
    ", 'Concord, NH ' : 'USA'\n",
    ", 'Connecticut' : 'USA'\n",
    ", 'Conroe, TX' : 'USA'\n",
    ", 'Coolidge, AZ' : 'USA'\n",
    ", 'CORNFIELDS' : 'USA'\n",
    ", 'Cornwall' : 'USA'\n",
    ", 'Corpus - Las Vegas - Houston' : 'USA'\n",
    ", 'Corpus Christi' : 'USA'\n",
    ", 'Corpus Christi, Texas' : 'USA'\n",
    ", 'Cosmic Oneness' : 'USA'\n",
    ", 'Costa Rica' : 'USA'\n",
    ", 'Cottonwood Arizona' : 'USA'\n",
    ", 'County Durham, United Kingdom' : 'UK'\n",
    ", 'Coventry' : 'USA'\n",
    ", 'Coventry, Rhode Island' : 'USA'\n",
    ", 'Coventry, UK' : 'UK'\n",
    ", 'CPT & JHB, South Africa' : 'ZAF'\n",
    ", 'Crato - CE ' : 'USA'\n",
    ", 'Crayford, London' : 'UK'\n",
    ", 'Crouch End, London' : 'UK'\n",
    ", 'Croydon' : 'USA'\n",
    ", 'CT & NY' : 'USA'\n",
    ", 'CT, USA' : 'USA'\n",
    ", 'D√å_sseldorf, Germany' : 'GER'\n",
    ", 'DaKounty, Pa' : 'USA'\n",
    ", 'dallas' : 'USA'\n",
    ", 'Dallas Fort-Worth' : 'USA'\n",
    ", 'Dallas, Tejas' : 'USA'\n",
    ", 'Dallas, Texas. ' : 'USA'\n",
    ", 'Dallas, TX' : 'USA'\n",
    ", 'Dallas, TX ' : 'USA'\n",
    ", 'Dammam- KSA' : 'SAU'\n",
    ", 'Danbury, CT' : 'USA'\n",
    ", 'Danville, VA' : 'USA'\n",
    ", 'Dappar (Mohali) Punjab' : 'IND'\n",
    ", 'Davao City' : 'USA'\n",
    ", 'Davidson, NC' : 'USA'\n",
    ", 'Davis, California' : 'USA'\n",
    ", 'Dayton, OH' : 'USA'\n",
    ", 'Dayton, Ohio' : 'USA'\n",
    ", 'DC' : 'USA'\n",
    ", 'DC Metro area' : 'USA'\n",
    ", 'DC, frequently NYC/San Diego' : 'USA'\n",
    ", 'Deadend, UK' : 'UK'\n",
    ", 'death star' : 'USA'\n",
    ", 'Decatur, GA' : 'USA'\n",
    ", 'Delhi ' : 'IND'\n",
    ", 'denmark' : 'DEN'\n",
    ", 'Denton, Texas' : 'USA'\n",
    ", 'denver colorado' : 'USA'\n",
    ", 'Denver Colorado. Fun Times' : 'USA'\n",
    ", 'Denver, CO' : 'USA'\n",
    ", 'Denver, Colorado' : 'USA'\n",
    ", 'Derbyshire, United Kingdom' : 'UK'\n",
    ", 'Des Moines, IA' : 'USA'\n",
    ", 'Des Moines, Iowa ' : 'USA'\n",
    ", 'Desde Republica Argentina' : 'ARG'\n",
    ", 'Desert Storm?? |BCHS|' : 'USA'\n",
    ", 'Detroit' : 'USA'\n",
    ", 'Detroit Tigers Dugout' : 'USA'\n",
    ", 'Detroit, MI' : 'USA'\n",
    ", 'Detroit, MI, United States' : 'USA'\n",
    ", 'Detroit, Michigan' : 'USA'\n",
    ", 'Detroit/Windsor' : 'USA'\n",
    ", 'Devon/London ' : 'UK'\n",
    ", 'DFW, Texas' : 'USA'\n",
    ", 'Dhaka' : 'BGD'\n",
    ", 'Dhaka, Bangladsh' : 'BGD'\n",
    ", 'Displaced Son of TEXAS!' : 'USA'\n",
    ", 'Dorset, UK' : 'UK'\n",
    ", 'Dorset, United Kingdom' : 'UK'\n",
    ", 'Dover, DE' : 'USA'\n",
    ", 'Downtown Churubusco, Indiana' : 'USA'\n",
    ", 'Downtown Oklahoma City' : 'USA'\n",
    ", 'Dreieich, Germany' : 'GER'\n",
    ", 'Dubai' : 'UAE'\n",
    ", 'dubai ' : 'UAE'\n",
    ", 'Dubai, UAE' : 'UAE'\n",
    ", 'Dubai, United Arab Emirates' : 'UAE'\n",
    ", 'Dublin' : 'IRL'\n",
    ", 'dublin ' : 'IRL'\n",
    ", 'Dublin City, Ireland' : 'IRL'\n",
    ", 'Dublin, Ireland' : 'IRL'\n",
    ", 'ducked off . . . ' : 'IRL'\n",
    ", 'Dudetown' : 'IRL'\n",
    ", 'Duncan' : 'IRL'\n",
    ", 'dundalk ireland' : 'IRL'\n",
    ", 'Dundas, Ontario' : 'CAN'\n",
    ", 'Dundee' : 'CAN'\n",
    ", 'Dundee, UK' : 'UK'\n",
    ", 'Dunwoody, GA' : 'USA'\n",
    ", 'Durand, MI' : 'USA'\n",
    ", 'Durban, South Africa' : 'ZAF'\n",
    ", 'Durham N.C ' : 'USA'\n",
    ", 'Durham, NC' : 'USA'\n",
    ", 'Dutch/English/German' : 'GER'\n",
    ", 'Duval, WV 25573, USA ?' : 'USA'\n",
    ", 'Eagle Mountain, Texas ' : 'USA'\n",
    ", 'Eagle Pass, Texas' : 'USA'\n",
    ", 'Eagle River Alaska' : 'USA'\n",
    ", 'Ealing, London' : 'UK'\n",
    ", 'East Atlanta, Georgia' : 'USA'\n",
    ", 'East Aurora, NY' : 'USA'\n",
    ", 'East Islip, NY' : 'USA'\n",
    ", 'East Lansing, MI' : 'USA'\n",
    ", 'East London' : 'UK'\n",
    ", 'East London. ' : 'UK'\n",
    ", 'EastAtlanta ??#WestGeorgia\\'18' : 'USA'\n",
    ", 'Eastbourne England' : 'UK'\n",
    ", 'EastCarolina' : 'USA'\n",
    ", 'Eastern Iowa' : 'USA'\n",
    ", 'Eastlake, OH' : 'USA'\n",
    ", 'Eau Claire, Wisconsin' : 'USA'\n",
    ", 'Eaubonne, 95, France' : 'FRA'\n",
    ", 'eBooks, North America' : 'USA'\n",
    ", 'Edinburgh' : 'UK'\n",
    ", 'Edinburgh, Scotland' : 'UK'\n",
    ", 'Edmonton, Alberta' : 'CAN'\n",
    ", 'Edmonton, Alberta - Treaty 6' : 'CAN'\n",
    ", 'EGYPT' : 'EGY'\n",
    ", 'EIU  Chucktown/LaSalle IL' : 'USA'\n",
    ", 'El Dorado, Arkansas' : 'USA'\n",
    ", 'El Dorado, KS' : 'USA'\n",
    ", 'El Paso, Texas' : 'USA'\n",
    ", 'El Paso, TX' : 'USA'\n",
    ", 'Elizabeth, NJ' : 'USA'\n",
    ", 'Elk Grove, CA, USA' : 'USA'\n",
    ", 'Elkhart, IN' : 'USA'\n",
    ", 'Ellensburg to Spokane' : 'USA'\n",
    ", 'Elmwood Park, NJ' : 'USA'\n",
    ", 'Elsewhere, NZ' : 'USA'\n",
    ", 'Emirates' : 'UAE'\n",
    ", 'Enfield, UK' : 'UK'\n",
    ", 'England' : 'UK'\n",
    ", 'England ' : 'UK'\n",
    ", 'England & Wales Border, UK' : 'UK'\n",
    ", 'England, Great Britain.' : 'UK'\n",
    ", 'England, United Kingdom' : 'UK'\n",
    ", 'England,UK,Europe,Sol 3.' : 'UK'\n",
    ", 'England.' : 'UK'\n",
    ", 'English Midlands' : 'UK'\n",
    ", 'Enterprise, Alabama' : 'USA'\n",
    ", 'Enterprise, NV' : 'USA'\n",
    ", 'Erbil' : 'IRQ'\n",
    ", 'Erie, PA' : 'USA'\n",
    ", 'Escondido, CA' : 'USA'\n",
    ", 'Espa√å¬±a - Spain - Espagne' : 'ESP'\n",
    ", 'Espa√å¬±a, Spain' : 'ESP'\n",
    ", 'Espoo, Finland' : 'FIN'\n",
    ", 'Essex' : 'UK'\n",
    ", 'Essex, England' : 'UK'\n",
    ", 'Essex/Brighton' : 'UK'\n",
    ", 'Est. September 2012 - Bristol' : 'UK'\n",
    ", 'Estados Unidos' : 'USA'\n",
    ", 'Eugene, Oregon' : 'USA'\n",
    ", 'Eureka, California, USA' : 'USA'\n",
    ", 'Evanston, IL' : 'USA'\n",
    ", 'Evansville, IN' : 'USA'\n",
    ", 'Everett, WA' : 'USA'\n",
    ", 'Evergreen Colorado' : 'USA'\n",
    ", 'Ewa Beach, HI' : 'USA'\n",
    ", 'Fairfax, VA' : 'USA'\n",
    ", 'Fairfield, California' : 'USA'\n",
    ", 'Fife, WA' : 'USA'\n",
    ", 'Finland' : 'FIN'\n",
    ", 'fl' : 'USA'\n",
    ", 'Fleet/Oxford, UK' : 'UK'\n",
    ", 'Florida' : 'USA'\n",
    ", 'Florida but I wanna be n Texas' : 'USA'\n",
    ", 'Florida Forever' : 'USA'\n",
    ", 'Florida USA' : 'USA'\n",
    ", 'Florida, USA' : 'USA'\n",
    ", 'Fort Calhoun, NE' : 'USA'\n",
    ", 'Fort Collins, CO' : 'USA'\n",
    ", 'Fort Fizz, Ohio' : 'USA'\n",
    ", 'Fort Knox, KY 40121' : 'USA'\n",
    ", 'Fort Lauderdale, FL' : 'USA'\n",
    ", 'Fort Myers, Florida' : 'USA'\n",
    ", 'Fort Smith, AR' : 'USA'\n",
    ", 'Fort Valley,GA/Fayetteville,AR' : 'USA'\n",
    ", 'Fort Walton Beach, Fl' : 'USA'\n",
    ", 'Fort Wayne, IN' : 'USA'\n",
    ", 'Fort Worth,  Texas ' : 'USA'\n",
    ", 'Fort Worth, Texas' : 'USA'\n",
    ", 'Fountain City, IN ' : 'USA'\n",
    ", 'Fountain Valley, CA' : 'USA'\n",
    ", 'France' : 'FRA'\n",
    ", 'Frankfort, KY' : 'USA'\n",
    ", 'Franklin, TN near Nashville' : 'USA'\n",
    ", 'Frascati' : 'USA'\n",
    ", 'Fredonia,NY' : 'USA'\n",
    ", 'Free State, South Africa' : 'ZAF'\n",
    ", 'Freeport IL. USA' : 'USA'\n",
    ", 'Freeport Ny' : 'USA'\n",
    ", 'Fresno, CA' : 'USA'\n",
    ", 'Fresno, California' : 'USA'\n",
    ", 'Friendswood, TX' : 'USA'\n",
    ", 'Frisco, TX' : 'USA'\n",
    ", 'From a torn up town MANCHESTER' : 'UK'\n",
    ", 'From NY. In Scranton, PA' : 'USA'\n",
    ", 'Frome, Somerset, England' : 'UK'\n",
    ", 'Fukuoka, Japan' : 'JPN'\n",
    ", 'Fukushima city Fukushima.pref' : 'JPN'\n",
    ", 'Gages Lake, IL' : 'USA'\n",
    ", 'Gainesville, FL' : 'USA'\n",
    ", 'Gainesville/Tampa, FL' : 'USA'\n",
    ", 'Galveston, Texas' : 'USA'\n",
    ", 'Garden City, NY' : 'USA'\n",
    ", 'Georgia, USA' : 'USA'\n",
    ", 'germany' : 'GER'\n",
    ", 'Gettysburg, PA' : 'USA'\n",
    ", 'Glendale, CA' : 'USA'\n",
    ", 'Gloucestershire , UK' : 'UK'\n",
    ", 'Goa, India' : 'IND'\n",
    ", 'Gold Coast, Australia' : 'AUS'\n",
    ", 'Gold Coast, Qld, Australia' : 'AUS'\n",
    ", 'Gotham City,USA' : 'USA'\n",
    ", 'Grand Rapids MI' : 'USA'\n",
    ", 'Greater Manchester, UK' : 'UK'\n",
    ", 'Greenfield, Massachusetts' : 'USA'\n",
    ", 'Greensboro, NC' : 'USA'\n",
    ", 'Greensburg, PA' : 'USA'\n",
    ", 'Greenville' : 'USA'\n",
    ", 'Greenville, S.C.' : 'USA'\n",
    ", 'Greenville,SC' : 'USA'\n",
    ", 'Guelph Ontario Canada' : 'CAN'\n",
    ", 'Guildford, UK' : 'UK'\n",
    ", 'Hackney, London' : 'UK'\n",
    ", 'Haddonfield, NJ' : 'USA'\n",
    ", 'Hagerstown, MD' : 'USA'\n",
    ", 'Haiku, Maui, Hawaii' : 'USA'\n",
    ", 'Hailing from Dayton ' : 'USA'\n",
    ", 'Halfrica' : 'USA'\n",
    ", 'Halifax' : 'USA'\n",
    ", 'Halifax, Nouvelle-√å√§cosse' : 'USA'\n",
    ", 'Halifax, Nova Scotia' : 'USA'\n",
    ", 'Halifax, NS, Canada' : 'CAN'\n",
    ", 'Halton Region' : 'USA'\n",
    ", 'Halton, Ontario' : 'CAN'\n",
    ", 'Hamburg, DE' : 'USA'\n",
    ", 'Hame' : 'USA'\n",
    ", 'Hamilton County, IN' : 'USA'\n",
    ", 'Hamilton, ON' : 'USA'\n",
    ", 'Hamilton, Ontario CA' : 'USA'\n",
    ", 'Hamilton, Ontario Canada' : 'CAN'\n",
    ", 'Hammersmith, London' : 'UK'\n",
    ", 'Hampshire UK' : 'UK'\n",
    ", 'Hampshire, UK' : 'UK'\n",
    ", 'Hampstead, London.' : 'UK'\n",
    ", 'Hampton Roads, VA' : 'USA'\n",
    ", 'Hannover, Germany' : 'GER'\n",
    ", 'Happily Married with 2 kids ' : 'USA'\n",
    ", 'Harbour Heights, FL' : 'USA'\n",
    ", 'Harlem, New York' : 'USA'\n",
    ", 'Harlem, NY or Chocolate City' : 'USA'\n",
    ", 'Harlingen, TX' : 'USA'\n",
    ", 'Harper Woods, MI' : 'USA'\n",
    ", 'Harpurhey, Manchester, UK' : 'UK'\n",
    ", 'Harris County, Texas' : 'USA'\n",
    ", 'Hartford,  connecticut' : 'USA'\n",
    ", 'Hartford, Connecticut' : 'USA'\n",
    ", 'hatena bookmark' : 'USA'\n",
    ", 'Hatteras, North Carolina' : 'USA'\n",
    ", 'Hattiesburg, MS' : 'USA'\n",
    ", 'Hawaii' : 'USA'\n",
    ", 'Hawaii USA' : 'USA'\n",
    ", 'Hawaii, USA' : 'USA'\n",
    ", 'Hawthorne, NE' : 'USA'\n",
    ", 'Haysville, KS' : 'USA'\n",
    ", 'Head Office: United Kingdom' : 'UK'\n",
    ", 'Heathrow' : 'UK'\n",
    ", 'Helsinki' : 'FIN'\n",
    ", 'Helsinki, Finland' : 'FIN'\n",
    ", 'Henderson, Nevada' : 'USA'\n",
    ", 'Henderson, NV' : 'USA'\n",
    ", 'Hendersonville, NC' : 'USA'\n",
    ", 'Hermitage, PA' : 'USA'\n",
    ", 'Hermosa Beach, CA' : 'USA'\n",
    ", 'Hickville, USA' : 'USA'\n",
    ", 'Highland Park, CA' : 'USA'\n",
    ", 'Hillsville/Lynchburg, VA' : 'USA'\n",
    ", 'Holland MI via Houston, CLE' : 'USA'\n",
    ", 'Holly Springs, NC ' : 'USA'\n",
    ", 'Holly, MI' : 'USA'\n",
    ", 'Hollywood' : 'USA'\n",
    ", 'Hollywood, CA' : 'USA'\n",
    ", 'hollywoodland ' : 'USA'\n",
    ", 'Honolulu, Hawaii' : 'USA'\n",
    ", 'Honolulu,Hawaii ' : 'USA'\n",
    ", 'Horsemind, MI' : 'USA'\n",
    ", 'houstn' : 'USA'\n",
    ", 'houston' : 'USA'\n",
    ", 'Houston ' : 'USA'\n",
    ", 'Houston |??| Corsicana' : 'USA'\n",
    ", 'Houston TX' : 'USA'\n",
    ", 'Houston,  TX' : 'USA'\n",
    ", 'Houston, Texas' : 'USA'\n",
    ", 'Houston, Texas ! ' : 'USA'\n",
    ", 'Houston, TX' : 'USA'\n",
    ", 'Houston, TX  ' : 'USA'\n",
    ", 'Hoxton, London' : 'UK'\n",
    ", 'Huber Heights, OH' : 'USA'\n",
    ", 'Hudson Valley, NY' : 'USA'\n",
    ", 'Hughes, AR' : 'USA'\n",
    ", 'Huntington, WV' : 'USA'\n",
    ", 'Huntley, IL' : 'USA'\n",
    ", 'Huntsville AL' : 'USA'\n",
    ", 'Huntsville, AL' : 'USA'\n",
    ", 'Huntsville, Alabama' : 'USA'\n",
    ", 'Hustletown' : 'USA'\n",
    ", 'hyderabad' : 'PAK'\n",
    ", 'Hyderabad Telangana INDIA' : 'IND'\n",
    ", 'I-75 in Florida' : 'USA'\n",
    ", 'IG : Sincerely_TSUNAMI' : 'USA'\n",
    ", 'Iliff,Colorado  ' : 'USA'\n",
    ", 'ill yorker' : 'USA'\n",
    ", 'Illinois' : 'USA'\n",
    ", 'Illinois, USA' : 'USA'\n",
    ", 'illinois. united state ' : 'USA'\n",
    ", 'Im Around ... Jersey' : 'USA'\n",
    ", 'India' : 'IND'\n",
    ", 'indiana' : 'USA'\n",
    ", 'Indiana, USA' : 'USA'\n",
    ", 'Indianapolis, IN' : 'USA'\n",
    ", 'IndiLand ' : 'USA'\n",
    ", 'Indonesia' : 'IDN'\n",
    ", 'Indonesia' : 'IDN'\n",
    ", 'Inglewood, CA' : 'USA'\n",
    ", 'Iowa, USA' : 'USA'\n",
    ", 'Ireland' : 'IRL'\n",
    ", 'Irving , Texas' : 'USA'\n",
    ", 'Islamabad' : 'PAK'\n",
    ", 'Island Lake, IL' : 'USA'\n",
    ", 'Isle of Man' : 'UK'\n",
    ", 'Istanbul' : 'TUR'\n",
    ", 'italy' : 'ITA'\n",
    ", 'Jacksonville Beach, FL' : 'USA'\n",
    ", 'Jacksonville, FL' : 'USA'\n",
    ", 'Jaipur, India' : 'IND'\n",
    ", 'Jaipur, Rajasthan, India' : 'IND'\n",
    ", 'Jakarta' : 'IDN'\n",
    ", 'Jakarta, Indonesia' : 'IDN'\n",
    ", 'Jakarta/Kuala Lumpur/S\\'pore' : 'IDN'\n",
    ", 'Japan' : 'JPN'\n",
    ", 'japon' : 'JPN'\n",
    ", 'Jeddah_Saudi Arabia.' : 'SAU'\n",
    ", 'Jersey' : 'USA'\n",
    ", 'jersey ' : 'USA'\n",
    ", 'Jersey - C.I' : 'USA'\n",
    ", 'Jersey City, New Jersey' : 'USA'\n",
    ", 'Jersey City, NJ' : 'USA'\n",
    ", 'Jersey Shore' : 'USA'\n",
    ", 'Jerseyville, IL' : 'USA'\n",
    ", 'Johannesburg, South Africa' : 'ZAF'\n",
    ", 'Johannesburg, South Africa ' : 'ZAF'\n",
    ", 'Jonesboro, AR MO, IOWA USA' : 'USA'\n",
    ", 'Jonesboro, Arkansas USA' : 'USA'\n",
    ", 'Joshua Tree, CA' : 'USA'\n",
    ", 'Jubail IC, Saudi Arabia' : 'SAU'\n",
    ", 'Jubail IC, Saudi Arabia.' : 'SAU'\n",
    ", 'Kalamazoo, Michigan' : 'USA'\n",
    ", 'Kalimantan Timur, Indonesia' : 'IDN'\n",
    ", 'Kama | 18 | France ' : 'FRA'\n",
    ", 'Kamloops, BC' : 'USA'\n",
    ", 'kansas' : 'USA'\n",
    ", 'Kansas City' : 'USA'\n",
    ", 'Kansas City, MO' : 'USA'\n",
    ", 'Kansas City, Mo.' : 'USA'\n",
    ", 'Kansas, The Free State! ~ KC' : 'USA'\n",
    ", 'Karachi' : 'PAK'\n",
    ", 'Karachi ' : 'PAK'\n",
    ", 'Karachi Pakistan' : 'PAK'\n",
    ", 'Karachi, Pakistan' : 'PAK'\n",
    ", 'Kashmir!' : 'PAK'\n",
    ", 'Kauai, Hawaii' : 'USA'\n",
    ", 'Kawartha Lakes, Ontario, Canad' : 'CAN'\n",
    ", 'Keighley, England' : 'UK'\n",
    ", 'Kelowna, BC' : 'USA'\n",
    ", 'Kenosha, WI 53143' : 'USA'\n",
    ", 'Kensington, MD' : 'USA'\n",
    ", 'Kent' : 'USA'\n",
    ", 'Kenton, Ohio' : 'USA'\n",
    ", 'Kentucky, USA' : 'USA'\n",
    ", 'Kenya' : 'KEN'\n",
    ", 'Kettering, OH' : 'USA'\n",
    ", 'Kingston, Pennsylvania' : 'USA'\n",
    ", 'Knoxville, TN' : 'USA'\n",
    ", 'Kodiak, AK' : 'USA'\n",
    ", 'Kokomo, In' : 'USA'\n",
    ", 'Kolkata, India' : 'IND'\n",
    ", 'Kutztown, PA' : 'USA'\n",
    ", 'L. A.' : 'USA'\n",
    ", 'La Grange Park, IL' : 'USA'\n",
    ", 'La Puente, CA' : 'USA'\n",
    ", 'LA/OC/Vegas' : 'USA'\n",
    ", 'Lagos' : 'NGA'\n",
    ", 'Lake Monticello, VA' : 'USA'\n",
    ", 'Lancashire, United Kingdom' : 'UK'\n",
    ", 'Lancaster California' : 'USA'\n",
    ", 'Lancaster, CA' : 'USA'\n",
    ", 'Lancaster, Pennsylvania, USA' : 'USA'\n",
    ", 'Laredo, TX' : 'USA'\n",
    ", 'Las Vegas' : 'USA'\n",
    ", 'Las Vegas aka Hell' : 'USA'\n",
    ", 'Las Vegas, Nevada' : 'USA'\n",
    ", 'Las Vegas, NV' : 'USA'\n",
    ", 'Las Vegas, NV ' : 'USA'\n",
    ", 'Las Vegas, NV USA' : 'USA'\n",
    ", 'Lawrence, KS via Emporia, KS' : 'USA'\n",
    ", 'LEALMAN, FLORIDA' : 'USA'\n",
    ", 'Leduc, Alberta, Canada' : 'CAN'\n",
    ", 'lee london' : 'UK'\n",
    ", 'Leeds' : 'USA'\n",
    ", 'Leeds, England' : 'USA'\n",
    ", 'Leeds, U.K.' : 'USA'\n",
    ", 'Leeds, UK' : 'UK'\n",
    ", 'Leeds, United Kingdom' : 'UK'\n",
    ", 'Leesburg, FL' : 'USA'\n",
    ", 'Lehigh Valley, PA' : 'USA'\n",
    ", 'Leicester' : 'UK'\n",
    ", 'Leicester, England' : 'UK'\n",
    ", 'Lethbridge, AB, Canada' : 'CAN'\n",
    ", 'Lethbridge, Alberta, Canada' : 'CAN'\n",
    ", 'Lima, OH' : 'USA'\n",
    ", 'Lima, Ohio' : 'USA'\n",
    ", 'Lincoln' : 'USA'\n",
    ", 'Lincoln City Oregon' : 'USA'\n",
    ", 'Lincoln, IL' : 'USA'\n",
    ", 'Lincoln, NE' : 'USA'\n",
    ", 'Lindenhurst' : 'USA'\n",
    ", 'Linton Hall, VA' : 'USA'\n",
    ", 'Lisbon, Portugal' : 'PRT'\n",
    ", 'Littleton, CO' : 'USA'\n",
    ", 'Littleton, CO, USA' : 'USA'\n",
    ", 'LITTLETON, CO, USA, TERRAN' : 'USA'\n",
    ", 'LiVE M√å¬ÅS' : 'USA'\n",
    ", 'Live m√å√Ås' : 'USA'\n",
    ", 'Live Oak, TX' : 'USA'\n",
    ", 'Live On Webcam' : 'USA'\n",
    ", 'LIVERPOOL' : 'UK'\n",
    ", 'liverpool ' : 'UK'\n",
    ", 'Lives in London' : 'UK'\n",
    ", 'Livingston, IL  U.S.A.' : 'USA'\n",
    ", 'Livingston, MT' : 'USA'\n",
    ", 'Livonia, MI' : 'USA'\n",
    ", 'Lizzy\\'s Knee' : 'USA'\n",
    ", 'London' : 'UK'\n",
    ", 'London ' : 'UK'\n",
    ", 'London / Birmingham' : 'UK'\n",
    ", 'london / st catharines ?' : 'UK'\n",
    ", 'london essex england uk' : 'UK'\n",
    ", 'london town..' : 'UK'\n",
    ", 'London UK' : 'UK'\n",
    ", 'London, England' : 'UK'\n",
    ", 'London, Greater London, UK' : 'UK'\n",
    ", 'London, Kent & SE England.' : 'UK'\n",
    ", 'London, UK' : 'UK'\n",
    ", 'London, United Kingdom' : 'UK'\n",
    ", 'London.' : 'UK'\n",
    ", 'London/Bristol/Guildford' : 'UK'\n",
    ", 'London/Lagos/FL √å√èT: 6.6200132,' : 'UK'\n",
    ", 'London/New York' : 'USA'\n",
    ", 'London/Outlaw Country ' : 'UK'\n",
    ", 'London/Surrey ' : 'UK'\n",
    ", 'Londonstan' : 'UK'\n",
    ", 'Long Beach, CA' : 'USA'\n",
    ", 'Long Eaton √•√° Derbyshire √•√° UK' : 'UK'\n",
    ", 'Long Island' : 'USA'\n",
    ", 'Long Island NY & San Francisco' : 'USA'\n",
    ", 'LONG ISLAND, NY' : 'USA'\n",
    ", 'Los Angeles' : 'USA'\n",
    ", 'Los Angeles ' : 'USA'\n",
    ", 'Los Angeles for now' : 'USA'\n",
    ", 'Los Angeles New York' : 'USA'\n",
    ", 'Los Angeles, CA' : 'USA'\n",
    ", 'Los Angeles, Calif.' : 'USA'\n",
    ", 'Los Angeles, California' : 'USA'\n",
    ", 'Los Angeles,CA, USA' : 'USA'\n",
    ", 'Los Angeles... CA... USA' : 'USA'\n",
    ", 'Los Angles, CA' : 'USA'\n",
    ", 'Louavul, KY' : 'USA'\n",
    ", 'Loughborough, England' : 'UK'\n",
    ", 'Loughborough.' : 'UK'\n",
    ", 'Loughton, Essex, UK' : 'UK'\n",
    ", 'Louisiana' : 'USA'\n",
    ", 'Louisiana, USA' : 'USA'\n",
    ", 'louisville, kentucky' : 'USA'\n",
    ", 'Louisville, KY' : 'USA'\n",
    ", 'Louisville, KY ' : 'USA'\n",
    ", 'Loveland Colorado' : 'USA'\n",
    ", 'Lowell, MA' : 'USA'\n",
    ", 'LP, MN USA' : 'USA'\n",
    ", 'Lubbock, Texas' : 'USA'\n",
    ", 'Lubbock, TX' : 'USA'\n",
    ", 'Lucknow, India' : 'IND'\n",
    ", 'Lynchburg, VA' : 'USA'\n",
    ", 'Lynwood, CA' : 'USA'\n",
    ", 'MA via PA' : 'USA'\n",
    ", 'Macclesfield' : 'USA'\n",
    ", 'Mackay, QLD, Australia' : 'AUS'\n",
    ", 'Mackem in Bolton' : 'USA'\n",
    ", 'Macon, GA' : 'USA'\n",
    ", 'Macon, Georgia' : 'USA'\n",
    ", 'MAD as Hell' : 'USA'\n",
    ", 'Made Here In Detroit ' : 'USA'\n",
    ", 'Made in America' : 'USA'\n",
    ", 'Madison, GA' : 'USA'\n",
    ", 'Madison, WI' : 'USA'\n",
    ", 'Madison, WI & St. Louis MO' : 'USA'\n",
    ", 'Madison, Wisconsin, USA' : 'USA'\n",
    ", 'Madisonville TN' : 'USA'\n",
    ", 'Madrid' : 'ESP'\n",
    ", 'Madrid, Comunidad de Madrid' : 'ESP'\n",
    ", 'mainly California' : 'USA'\n",
    ", 'Manchester' : 'UK'\n",
    ", 'Manchester UK' : 'UK'\n",
    ", 'Manchester, England' : 'UK'\n",
    ", 'Manchester, NH' : 'USA'\n",
    ", 'Manchester, UK' : 'UK'\n",
    ", 'manchester, uk.' : 'UK'\n",
    ", 'Manhattan' : 'USA'\n",
    ", 'Manhattan, NY' : 'USA'\n",
    ", 'Marbella. Spain' : 'ESP'\n",
    ", 'Maricopa, AZ' : 'USA'\n",
    ", 'Maryland' : 'USA'\n",
    ", 'Maryland ' : 'USA'\n",
    ", 'Maryland, USA' : 'USA'\n",
    ", 'Maryland,Baltimore' : 'USA'\n",
    ", 'marysville ca ' : 'USA'\n",
    ", 'Marysville, MI' : 'USA'\n",
    ", 'Mass' : 'USA'\n",
    ", 'Massachusetts' : 'USA'\n",
    ", 'Massachusetts ' : 'USA'\n",
    ", 'Massachusetts, USA' : 'USA'\n",
    ", 'McLean, VA' : 'USA'\n",
    ", 'Medford, NJ' : 'USA'\n",
    ", 'Medford, Oregon' : 'USA'\n",
    ", 'Meereen ' : 'USA'\n",
    ", 'melbourne' : 'AUS'\n",
    ", 'Melbourne Australia' : 'AUS'\n",
    ", 'Melbourne-ish' : 'AUS'\n",
    ", 'Melbourne, Australia' : 'AUS'\n",
    ", 'Melbourne, Australia.' : 'AUS'\n",
    ", 'Melbourne, FL' : 'USA'\n",
    ", 'Melbourne, Florida' : 'USA'\n",
    ", 'Melbourne, Victoria' : 'AUS'\n",
    ", 'Melrose' : 'AUS'\n",
    ", 'Melton, GA' : 'USA'\n",
    ", 'Memphis' : 'USA'\n",
    ", 'Memphis, in the Tennessees' : 'USA'\n",
    ", 'Memphis, TN' : 'USA'\n",
    ", 'Menasha, WI' : 'USA'\n",
    ", 'Mentor OH' : 'USA'\n",
    ", 'Merica!' : 'USA'\n",
    ", 'Mesa, AZ' : 'USA'\n",
    ", 'Methville, CA' : 'USA'\n",
    ", 'Metro Manila' : 'USA'\n",
    ", 'mexico' : 'USA'\n",
    ", 'Mexico City' : 'USA'\n",
    ", 'Mexico! ^_^' : 'USA'\n",
    ", 'MI' : 'USA'\n",
    ", 'MI - CA' : 'USA'\n",
    ", 'MI,USA' : 'USA'\n",
    ", 'Miami' : 'USA'\n",
    ", 'Miami ??' : 'USA'\n",
    ", 'Miami Beach, Fl' : 'USA'\n",
    ", 'Miami via Lima' : 'USA'\n",
    ", 'miami x dallas ' : 'USA'\n",
    ", 'Miami, FL' : 'USA'\n",
    ", 'Miami, Florida' : 'USA'\n",
    ", 'Miami,FL' : 'USA'\n",
    ", 'Miami,Fla' : 'USA'\n",
    ", 'Miami?Gainesville' : 'USA'\n",
    ", 'Michel Delving.' : 'USA'\n",
    ", 'Michigan' : 'USA'\n",
    ", 'Michigan ' : 'USA'\n",
    ", 'Michigan, USA' : 'USA'\n",
    ", 'Mid north coast of NSW' : 'USA'\n",
    ", 'Mid West' : 'USA'\n",
    ", 'Middle Earth / Asgard / Berk' : 'USA'\n",
    ", 'middle eastern palace' : 'USA'\n",
    ", 'midwest' : 'USA'\n",
    ", 'Midwest City, OK' : 'USA'\n",
    ", 'Midwestern USA' : 'USA'\n",
    ", 'milky way' : 'USA'\n",
    ", 'Milky Way galaxy ' : 'USA'\n",
    ", 'Milton keynes' : 'USA'\n",
    ", 'Milton Keynes ' : 'USA'\n",
    ", 'Milton Keynes, England' : 'UK'\n",
    ", 'Milton/Tallahassee' : 'USA'\n",
    ", 'Milwaukee County' : 'USA'\n",
    ", 'Milwaukee WI' : 'USA'\n",
    ", 'Milwaukee, WI' : 'USA'\n",
    ", 'Minneapolis - St. Paul' : 'USA'\n",
    ", 'Minneapolis, MN' : 'USA'\n",
    ", 'Minneapolis,MN,US' : 'USA'\n",
    ", 'Minneapolis/St. Paul' : 'USA'\n",
    ", 'Minority Privilege, USA' : 'USA'\n",
    ", 'Mississauga, Ontario' : 'CAN'\n",
    ", 'missouri USA' : 'USA'\n",
    ", 'Missouri, USA' : 'USA'\n",
    ", 'Mogadishu, New Jersey' : 'USA'\n",
    ", 'Montana, USA' : 'USA'\n",
    ", 'Mooresville, NC' : 'USA'\n",
    ", 'Morganville, Texas.' : 'USA'\n",
    ", 'Morioh, Japan' : 'JPN'\n",
    ", 'Morris, IL' : 'USA'\n",
    ", 'Mount Vernon, NY' : 'USA'\n",
    ", 'Mumbai' : 'IND'\n",
    ", 'Mumbai , India' : 'IND'\n",
    ", 'Mumbai india' : 'IND'\n",
    ", 'Mumbai, India' : 'IND'\n",
    ", 'Murray Hill, New Jersey' : 'USA'\n",
    ", 'N. California USA' : 'USA'\n",
    ", 'Nairobi' : 'KEN'\n",
    ", 'NAIROBI  KENYA ' : 'KEN'\n",
    ", 'Nairobi , Kenya' : 'KEN'\n",
    ", 'Nairobi-KENYA' : 'KEN'\n",
    ", 'Nairobi, Kenya' : 'KEN'\n",
    ", 'Nairobi, Kenya ' : 'KEN'\n",
    ", 'Nakhon Si Thammarat' : 'KEN'\n",
    ", 'Nanaimo, BC, Canada' : 'CAN'\n",
    ", 'Nantes, France' : 'FRA'\n",
    ", 'Napa, CA' : 'USA'\n",
    ", 'Nashua NH' : 'USA'\n",
    ", 'Nashville' : 'USA'\n",
    ", 'Nashville, Tennessee' : 'USA'\n",
    ", 'Nashville, TN' : 'USA'\n",
    ", 'nashville, tn ' : 'USA'\n",
    ", 'nbc washington' : 'USA'\n",
    ", 'NC' : 'USA'\n",
    ", 'Near Richmond, VA' : 'USA'\n",
    ", 'Nelspruit, South Africa' : 'ZAF'\n",
    ", 'Netherlands' : 'NZL'\n",
    ", 'Netherlands,Amsterdam-Virtual ' : 'NZL'\n",
    ", 'Nevada (wishing for Colorado)' : 'USA'\n",
    ", 'Nevada, USA' : 'USA'\n",
    ", 'New Britain, CT' : 'USA'\n",
    ", 'New Brunswick, NJ' : 'USA'\n",
    ", 'New Chicago' : 'USA'\n",
    ", 'New Delhi' : 'IND'\n",
    ", 'New Delhi, Delhi' : 'IND'\n",
    ", 'New Delhi, India' : 'IND'\n",
    ", 'New Delhi,India' : 'IND'\n",
    ", 'New England' : 'UK'\n",
    ", 'New Hampshire' : 'UK'\n",
    ", 'New Hampshire, USA' : 'USA'\n",
    ", 'New Hanover County, NC' : 'USA'\n",
    ", 'New Haven, Connecticut' : 'USA'\n",
    ", 'New Jersey' : 'USA'\n",
    ", 'New Jersey ' : 'USA'\n",
    ", 'New Jersey, USA' : 'USA'\n",
    ", 'New Jersey, usually' : 'USA'\n",
    ", 'New Jersey/ D.R.' : 'USA'\n",
    ", 'New Jersey/New York' : 'USA'\n",
    ", 'New Mexico, USA' : 'USA'\n",
    ", 'New Orleans ,Louisiana' : 'USA'\n",
    ", 'New Orleans, LA' : 'USA'\n",
    ", 'New Orleans, Louisiana' : 'USA'\n",
    ", 'New South Wales, Australia' : 'AUS'\n",
    ", 'New Sweden' : 'SWE'\n",
    ", 'New York' : 'USA'\n",
    ", 'New York ' : 'USA'\n",
    ", 'New York - Connecticut' : 'USA'\n",
    ", 'New York ? ATL' : 'USA'\n",
    ", 'New York / Worldwide' : 'USA'\n",
    ", 'New York 2099' : 'USA'\n",
    ", 'New York Brooklyn' : 'USA'\n",
    ", 'New York City' : 'USA'\n",
    ", 'New York City ,NY' : 'USA'\n",
    ", 'New York City, NY' : 'USA'\n",
    ", 'New York NYC' : 'USA'\n",
    ", 'New York, New York' : 'USA'\n",
    ", 'New York, NY' : 'USA'\n",
    ", 'New York, NY ' : 'USA'\n",
    ", 'New York, United States' : 'USA'\n",
    ", 'New York, USA' : 'USA'\n",
    ", 'New York. NY' : 'USA'\n",
    ", 'New Your' : 'USA'\n",
    ", 'New Zealand' : 'NZL'\n",
    ", 'Newark, NJ' : 'USA'\n",
    ", 'Newcastle' : 'UK'\n",
    ", 'Newcastle upon Tyne' : 'USA'\n",
    ", 'Newcastle Upon Tyne, England' : 'UK'\n",
    ", 'Newcastle, England' : 'UK'\n",
    ", 'Newcastle, England ' : 'UK'\n",
    ", 'Newcastle, OK' : 'USA'\n",
    ", 'Newport, Wales, UK' : 'UK'\n",
    ", 'Newton Centre, Massachusetts' : 'USA'\n",
    ", 'Newton, NJ 07860' : 'USA'\n",
    ", 'NH via Boston, MA' : 'USA'\n",
    ", 'Nicoma Park, OK' : 'USA'\n",
    ", 'Nigeria' : 'NGA'\n",
    ", 'nj/ny' : 'USA'\n",
    ", 'Noida, NCR, India' : 'IND'\n",
    ", 'NOLA ?? TX' : 'USA'\n",
    ", 'Nomad, USA' : 'USA'\n",
    ", 'Norman, Oklahoma' : 'USA'\n",
    ", 'North America' : 'USA'\n",
    ", 'North Carolina, USA' : 'USA'\n",
    ", 'North Dartmouth, Massachusetts' : 'USA'\n",
    ", 'North East / Middlesbrough ' : 'UK'\n",
    ", 'North East Unsigned Radio' : 'UK'\n",
    ", 'North East USA' : 'USA'\n",
    ", 'North East, England' : 'UK'\n",
    ", 'North Ferriby, East Yorkshire' : 'UK'\n",
    ", 'North Highlands, CA' : 'USA'\n",
    ", 'North Jersey' : 'USA'\n",
    ", 'North London' : 'UK'\n",
    ", 'North Port, FL' : 'USA'\n",
    ", 'North Vancouver, BC' : 'CAN'\n",
    ", 'North West England UK' : 'UK'\n",
    ", 'North West London' : 'UK'\n",
    ", 'Northampton, MA' : 'USA'\n",
    ", 'Northern California U.S.A.' : 'USA'\n",
    ", 'Northern Colorado' : 'USA'\n",
    ", 'Northern Ireland' : 'USA'\n",
    ", 'Northern Kentucky, USA' : 'USA'\n",
    ", 'Norwalk, CT' : 'USA'\n",
    ", 'Not Los Angeles, Not New York.' : 'USA'\n",
    ", 'Nottingham, United Kingdom' : 'UK'\n",
    ", 'Nova Scotia, Canada' : 'CAN'\n",
    ", 'Novi, MI' : 'USA'\n",
    ", 'Numa casa de old yellow bricks' : 'USA'\n",
    ", 'NY' : 'USA'\n",
    ", 'NY || live easy? ' : 'USA'\n",
    ", 'NY Capital District' : 'USA'\n",
    ", 'NY, CT & Greece' : 'USA'\n",
    ", 'NY, NY' : 'USA'\n",
    ", 'nyc' : 'USA'\n",
    ", 'NYC :) Ex- #Islamophobe' : 'USA'\n",
    ", 'NYC / International' : 'USA'\n",
    ", 'NYC area' : 'USA'\n",
    ", 'NYC metro' : 'USA'\n",
    ", 'NYC-LA-MIAMI' : 'USA'\n",
    ", 'NYC, New York' : 'USA'\n",
    ", 'NYC,US - Cali, Colombia' : 'USA'\n",
    ", 'NYC&NJ' : 'USA'\n",
    ", 'NYHC' : 'USA'\n",
    ", 'Oakland, CA' : 'USA'\n",
    ", 'Ocean City, NJ' : 'USA'\n",
    ", 'Odawara, Japan' : 'JPN'\n",
    ", 'OES 4th Point. sisSTAR & TI' : 'USA'\n",
    ", 'Ohio' : 'USA'\n",
    ", 'Ohio, USA' : 'USA'\n",
    ", 'OK' : 'USA'\n",
    ", 'Okanagan Valley, BC' : 'USA'\n",
    ", 'Oklahoma' : 'USA'\n",
    ", 'Oklahoma City' : 'USA'\n",
    ", 'Oklahoma City, OK' : 'USA'\n",
    ", 'Oklahoma, USA' : 'USA'\n",
    ", 'Olathe, KS' : 'USA'\n",
    ", 'Oldenburg // London' : 'UK'\n",
    ", 'Olympia, WA' : 'USA'\n",
    ", 'Oneonta, NY/ Staten Island, NY' : 'USA'\n",
    ", 'Ontario Canada' : 'CAN'\n",
    ", 'Ontario, Canada' : 'CAN'\n",
    ", 'Ontario, Canada. ' : 'CAN'\n",
    ", 'Orange County, CA' : 'USA'\n",
    ", 'Orange County, California' : 'USA'\n",
    ", 'Orange County, NY' : 'USA'\n",
    ", 'Orbost, Victoria, Australia' : 'AUS'\n",
    ", 'Oregon and Washington' : 'USA'\n",
    ", 'Oregon, USA' : 'USA'\n",
    ", 'Orlando' : 'USA'\n",
    ", 'Orlando ' : 'USA'\n",
    ", 'Orlando, FL' : 'USA'\n",
    ", 'Orlando,FL  USA' : 'USA'\n",
    ", 'Orlando/Cocoa Beach, FL' : 'USA'\n",
    ", 'Ormond By The Sea, FL' : 'USA'\n",
    ", 'Oshawa, Canada' : 'CAN'\n",
    ", 'Oshawa/Toronto' : 'CAN'\n",
    ", 'Otsego, MI' : 'USA'\n",
    ", 'Ottawa, Canada' : 'CAN'\n",
    ", 'Ottawa, Ontario' : 'CAN'\n",
    ", 'Ottawa,Ontario Canada' : 'CAN'\n",
    ", 'Overland Park, KS' : 'USA'\n",
    ", 'oxford' : 'USA'\n",
    ", 'Oxford / bristol' : 'UK'\n",
    ", 'Oxford, MS' : 'USA'\n",
    ", 'Oxford, OH' : 'USA'\n",
    ", 'PA' : 'USA'\n",
    ", 'PA, USA' : 'USA'\n",
    ", 'PA.USA' : 'USA'\n",
    ", 'Pacific Northwest' : 'USA'\n",
    ", 'Paducah, KY' : 'USA'\n",
    ", 'Palestine Texas' : 'USA'\n",
    ", 'Palm Beach County, FL' : 'USA'\n",
    ", 'Palm Desert, CA' : 'USA'\n",
    ", 'Palo Alto, CA' : 'USA'\n",
    ", 'Palo Alto, California' : 'USA'\n",
    ", 'paradise' : 'USA'\n",
    ", 'Paradise City' : 'USA'\n",
    ", 'Paradise, NV' : 'USA'\n",
    ", 'Paranaque City' : 'USA'\n",
    ", 'Paris' : 'FRA'\n",
    ", 'Paris ' : 'FRA'\n",
    ", 'Paris (France)' : 'FRA'\n",
    ", 'Paris, France' : 'FRA'\n",
    ", 'Paris.' : 'FRA'\n",
    ", 'Park Ridge, Illinois' : 'USA'\n",
    ", 'Paterson, New Jersey ' : 'USA'\n",
    ", 'peekskill. new york, 10566 ' : 'USA'\n",
    ", 'Penn Hills, PA' : 'USA'\n",
    ", 'Pennsylvania, PA' : 'USA'\n",
    ", 'Pennsylvania, USA' : 'USA'\n",
    ", 'Pensacola, FL' : 'USA'\n",
    ", 'Perth, Australia' : 'AUS'\n",
    ", 'perth, australia ' : 'AUS'\n",
    ", 'Perth, Western Australia' : 'AUS'\n",
    ", 'Petaluma, CA' : 'USA'\n",
    ", 'Peterborough, Ontario, Canada' : 'CAN'\n",
    ", 'pettyville, usa' : 'USA'\n",
    ", 'ph' : 'USA'\n",
    ", 'Phila.' : 'USA'\n",
    ", 'Philadelphia' : 'USA'\n",
    ", 'Philadelphia, PA' : 'USA'\n",
    ", 'Philadelphia, PA ' : 'USA'\n",
    ", 'Philadelphia, PA USA' : 'USA'\n",
    ", 'Philadelphia, Pennsylvania' : 'USA'\n",
    ", 'Philadelphia, Pennsylvania USA' : 'USA'\n",
    ", 'philly' : 'USA'\n",
    ", 'philly ' : 'USA'\n",
    ", 'Phoenix' : 'USA'\n",
    ", 'Phoenix Az' : 'USA'\n",
    ", 'Phoenix, Arizona, USA' : 'USA'\n",
    ", 'Phoenix, AZ' : 'USA'\n",
    ", 'Piedmont Triad, NC' : 'USA'\n",
    ", 'Pig Symbol, Alabama' : 'USA'\n",
    ", 'Pittsburgh PA' : 'USA'\n",
    ", 'Plain O\\' Texas' : 'USA'\n",
    ", 'Plano, IL' : 'USA'\n",
    ", 'Plano, Texas' : 'USA'\n",
    ", 'Plano,TX' : 'USA'\n",
    ", 'Pleasanton, CA' : 'USA'\n",
    ", 'Pompano Beach, FL' : 'USA'\n",
    ", 'Pontefract UK' : 'UK'\n",
    ", 'Poplar, London' : 'UK'\n",
    ", 'Port Charlotte, FL' : 'USA'\n",
    ", 'Port Jervis, NY' : 'USA'\n",
    ", 'port matilda pa' : 'USA'\n",
    ", 'Port Orange, FL' : 'USA'\n",
    ", 'Portland, OR' : 'USA'\n",
    ", 'Portsmouth, UK' : 'UK'\n",
    ", 'Portsmouth, VA' : 'USA'\n",
    ", 'proudly South African' : 'ZAF'\n",
    ", 'Purgatory, USA' : 'USA'\n",
    ", 'QLD Australia' : 'AUS'\n",
    ", 'Quantico, VA' : 'USA'\n",
    ", 'Queen Creek AZ' : 'USA'\n",
    ", 'Queens New York' : 'USA'\n",
    ", 'Queens, NY' : 'USA'\n",
    ", 'Queensland, Australia' : 'AUS'\n",
    ", 'Raleigh (Garner/Cleveland) NC' : 'USA'\n",
    ", 'Raleigh Durham, NC' : 'USA'\n",
    ", 'Raleigh, NC' : 'USA'\n",
    ", 'Rapid City, Black Hills, SD' : 'USA'\n",
    ", 'Rapid City, South Dakota' : 'USA'\n",
    ", 'Reading UK' : 'UK'\n",
    ", 'Redding, California, USA' : 'USA'\n",
    ", 'Redondo Beach, CA' : 'USA'\n",
    ", 'Renfrew, Scotland' : 'UK'\n",
    ", 'Republic of Texas' : 'USA'\n",
    ", 'Reston, VA, USA' : 'USA'\n",
    ", 'Rheinbach / Germany' : 'GER'\n",
    ", 'Rhode Island' : 'USA'\n",
    ", 'RhodeIsland' : 'USA'\n",
    ", 'Richardson TX' : 'USA'\n",
    ", 'Richmond, VA' : 'USA'\n",
    ", 'rio de janeiro | brazil' : 'BRA'\n",
    ", 'Riverside, CA' : 'USA'\n",
    ", 'Riverside, California.' : 'USA'\n",
    ", 'Riverview, FL ' : 'USA'\n",
    ", 'Roanoke VA' : 'USA'\n",
    ", 'Roanoke, VA' : 'USA'\n",
    ", 'Rochester Hills, MI' : 'USA'\n",
    ", 'Rochester, NY' : 'USA'\n",
    ", 'Rockford, IL' : 'USA'\n",
    ", 'Rockland County, NY' : 'USA'\n",
    ", 'Rome, Italy' : 'ITA'\n",
    ", 'rowyso dallas ' : 'USA'\n",
    ", 'Rutherfordton, NC' : 'USA'\n",
    ", 'S√å¬£o Paulo SP,  Brasil' : 'BRA'\n",
    ", 'S√å¬£o Paulo, Brasil' : 'BRA'\n",
    ", 'Sacramento, CA' : 'USA'\n",
    ", 'Sacramento, California' : 'USA'\n",
    ", 'Saint Louis, Missouri' : 'USA'\n",
    ", 'Saint Lucia' : 'USA'\n",
    ", 'Saint Marys, GA' : 'USA'\n",
    ", 'Saint Paul' : 'USA'\n",
    ", 'Saipan, CNMI' : 'USA'\n",
    ", 'Saline, MI' : 'USA'\n",
    ", 'San Antonio-ish, TX' : 'USA'\n",
    ", 'San Antonio, TX' : 'USA'\n",
    ", 'San Diego' : 'USA'\n",
    ", 'San Diego CA' : 'USA'\n",
    ", 'San Diego California 92101' : 'USA'\n",
    ", 'San Diego, CA' : 'USA'\n",
    ", 'San Diego, Calif.' : 'USA'\n",
    ", 'San Diego, California' : 'USA'\n",
    ", 'San Diego, Texas.' : 'USA'\n",
    ", 'San Francisco' : 'USA'\n",
    ", 'San Francisco , CA' : 'USA'\n",
    ", 'San Francisco Bay Area' : 'USA'\n",
    ", 'San Francisco, CA' : 'USA'\n",
    ", 'San Fransokyo' : 'USA'\n",
    ", 'san gabriel la union' : 'USA'\n",
    ", 'San Jose' : 'USA'\n",
    ", 'San Jose, CA' : 'USA'\n",
    ", 'San Jose, CA, USA' : 'USA'\n",
    ", 'San Jose, California' : 'USA'\n",
    ", 'San Luis Obispo, CA' : 'USA'\n",
    ", 'San Mateo County, CA' : 'USA'\n",
    ", 'Sand springs oklahoma' : 'USA'\n",
    ", 'Sandton, South Africa' : 'ZAF'\n",
    ", 'Santa Clara, CA' : 'USA'\n",
    ", 'Santa Cruz, CA' : 'USA'\n",
    ", 'Santa Maria, CA' : 'USA'\n",
    ", 'Santa Monica, CA' : 'USA'\n",
    ", 'Sao Paulo, Brazil' : 'BRA'\n",
    ", 'Sarasota, FL' : 'USA'\n",
    ", 'Saskatchewan, Canada' : 'CAN'\n",
    ", 'Saudi Arabia' : 'SAU'\n",
    ", 'SaudI arabia - riyadh ' : 'SAU'\n",
    ", 'Savage States of America' : 'USA'\n",
    ", 'Scotland' : 'UK'\n",
    ", 'Scotland, United Kingdom' : 'UK'\n",
    ", 'Scotts Valley, CA' : 'USA'\n",
    ", 'Scottsdale, AZ' : 'USA'\n",
    ", 'Scottsdale. AZ' : 'USA'\n",
    ", 'Screwston, TX' : 'USA'\n",
    ", 'SE London(heart is by the sea)' : 'UK'\n",
    ", 'Seattle' : 'USA'\n",
    ", 'Seattle native in Prescott, AZ' : 'USA'\n",
    ", 'Seattle, WA' : 'USA'\n",
    ", 'SEATTLE, WA USA' : 'USA'\n",
    ", 'Seattle, Washington' : 'USA'\n",
    ", 'SF Bay Area, California / Greater Phoenix, AZ' : 'USA'\n",
    ", 'Sharkatraz/Bindle\\'s Cleft, PA' : 'USA'\n",
    ", 'Sherwood, Brisbane, Australia' : 'AUS'\n",
    ", 'Shirley, NY' : 'USA'\n",
    ", 'Singapore' : 'SGP'\n",
    ", 'sitting on the fence, New York' : 'USA'\n",
    ", 'Somerset, UK' : 'UK'\n",
    ", 'Somewhere between Chicago & Milwaukee' : 'USA'\n",
    ", 'Somewhere in China.' : 'CHN'\n",
    ", 'Somewhere in Jersey' : 'USA'\n",
    ", 'Somewhere in Spain' : 'ESP'\n",
    ", 'Somewhere in the Canada' : 'CAN'\n",
    ", 'somewhere USA ' : 'USA'\n",
    ", 'South Africa' : 'ZAF'\n",
    ", 'south africa eastern cape' : 'ZAF'\n",
    ", 'South Bloomfield, OH' : 'USA'\n",
    ", 'South Carolina, USA' : 'USA'\n",
    ", 'South Florida' : 'USA'\n",
    ", 'South Pasadena, CA' : 'USA'\n",
    ", 'South, USA' : 'USA'\n",
    ", 'Southern California' : 'USA'\n",
    ", 'SOUTHERN CALIFORNIA DESERT' : 'USA'\n",
    ", 'southwest, Tx' : 'USA'\n",
    ", 'Spain' : 'ESP'\n",
    ", 'Spain but Opa-Locka, FL' : 'USA'\n",
    ", 'Spokane, Washington' : 'USA'\n",
    ", 'Spokane, Washington 99206' : 'USA'\n",
    ", 'Spring Grove, IL' : 'USA'\n",
    ", 'Spring Tx' : 'USA'\n",
    ", 'St Austell, Cornwall' : 'USA'\n",
    ", 'St Charles, MD' : 'USA'\n",
    ", 'St Louis, MO' : 'USA'\n",
    ", 'St Paul, MN' : 'USA'\n",
    ", 'St PetersburgFL' : 'USA'\n",
    ", 'St. Catharines, Ontario' : 'CAN'\n",
    ", 'St. John\\'s, NL, Canada' : 'CAN'\n",
    ", 'St. Joseph, Minnesota' : 'USA'\n",
    ", 'St. Louis' : 'USA'\n",
    ", 'St. Louis Mo.' : 'USA'\n",
    ", 'St. Louis, Missouri' : 'USA'\n",
    ", 'St. Louis, MO' : 'USA'\n",
    ", 'St. Patrick\\'s Purgatory' : 'USA'\n",
    ", 'St.Cloud, MN' : 'USA'\n",
    ", 'State College, PA' : 'USA'\n",
    ", 'Stockton on tees Teesside UK' : 'UK'\n",
    ", 'Street of Dallas' : 'USA'\n",
    ", 'Sugar Land, TX' : 'USA'\n",
    ", 'Sunny South florida ' : 'USA'\n",
    ", 'Sunnyvale, CA' : 'USA'\n",
    ", 'Sutton, London UK' : 'UK'\n",
    ", 'Sydney' : 'AUS'\n",
    ", 'Sydney Australia' : 'AUS'\n",
    ", 'Sydney, Australia' : 'AUS'\n",
    ", 'Sylacauga, Alabama' : 'USA'\n",
    ", 'Tacoma,Washington' : 'USA'\n",
    ", 'Tallahassee Florida' : 'USA'\n",
    ", 'Tallahassee, FL' : 'USA'\n",
    ", 'Tampa' : 'USA'\n",
    ", 'Tampa-St. Petersburg, FL' : 'USA'\n",
    ", 'Tampa, FL' : 'USA'\n",
    ", 'Temecula, CA' : 'USA'\n",
    ", 'Tennessee' : 'USA'\n",
    ", 'Tennessee, USA' : 'USA'\n",
    ", 'Terlingua, Texas' : 'USA'\n",
    ", 'Texas' : 'USA'\n",
    ", 'Texas ' : 'USA'\n",
    ", 'texas a&m university' : 'USA'\n",
    ", 'Texas af' : 'USA'\n",
    ", 'Texas-USA¬â√£¬¢ ?' : 'USA'\n",
    ", 'Texas, USA' : 'USA'\n",
    ", 'texasss' : 'USA'\n",
    ", 'The Great State of Texas' : 'USA'\n",
    ", 'The Hammock, FL, USA' : 'USA'\n",
    ", 'The land of New Jersey. ' : 'USA'\n",
    ", 'The UK' : 'UK'\n",
    ", 'The windy plains of Denver' : 'USA'\n",
    ", 'Topeka, KS' : 'USA'\n",
    ", 'Tornado Alley, USA ' : 'USA'\n",
    ", 'Toronto' : 'CAN'\n",
    ", 'toronto ¬â√õ¬¢ dallas' : 'USA'\n",
    ", 'Toronto-Citizen of Canada & US' : 'CAN'\n",
    ", 'Toronto, Bob-Lo, Miami Beach' : 'CAN'\n",
    ", 'Toronto, Canada' : 'CAN'\n",
    ", 'Toronto, ON' : 'CAN'\n",
    ", 'Toronto, ON, Canada' : 'CAN'\n",
    ", 'Toronto, Ontario' : 'CAN'\n",
    ", 'Torrance, CA' : 'USA'\n",
    ", 'Trackside California' : 'USA'\n",
    ", 'Tractor land aka Bristol' : 'UK'\n",
    ", 'trapped in America' : 'USA'\n",
    ", 'Traverse City, MI' : 'USA'\n",
    ", 'Tri-Cities, Wash.' : 'USA'\n",
    ", 'Tring, UK' : 'UK'\n",
    ", 'Trinity, Bailiwick of Jersey' : 'USA'\n",
    ", 'Tucson, Az' : 'USA'\n",
    ", 'Tulalip, Washington' : 'USA'\n",
    ", 'Tulsa, Oklahoma' : 'USA'\n",
    ", 'TX' : 'USA'\n",
    ", 'Tyler, TX' : 'USA'\n",
    ", 'U.K.' : 'UK'\n",
    ", 'U.S' : 'USA'\n",
    ", 'U.S.' : 'USA'\n",
    ", 'U.S. Northern Virginia' : 'USA'\n",
    ", 'U.S.A' : 'USA'\n",
    ", 'U.S.A.   FEMA Region 5' : 'USA'\n",
    ", 'U.S.A. - Global Members Site' : 'USA'\n",
    ", 'UAE,Sharjah/ AbuDhabi' : 'UAE'\n",
    ", 'UK' : 'UK'\n",
    ", 'UK Great Britain ' : 'UK'\n",
    ", 'Ukraine' : 'UKR'\n",
    ", 'United Kingdom' : 'UK'\n",
    ", 'United Kingdom,Fraserburgh' : 'UK'\n",
    ", 'United States' : 'USA'\n",
    ", 'United States of America' : 'USA'\n",
    ", 'United States where it\\'s warm' : 'USA'\n",
    ", 'Unites States' : 'USA'\n",
    ", 'University of Chicago' : 'USA'\n",
    ", 'University of South Florida' : 'USA'\n",
    ", 'University of Toronto' : 'CAN'\n",
    ", 'Upper manhattan, New York' : 'USA'\n",
    ", 'Upper St Clair, PA' : 'USA'\n",
    ", 'Upstate New York' : 'USA'\n",
    ", 'US' : 'USA'\n",
    ", 'US, PA' : 'USA'\n",
    ", 'USA' : 'USA'\n",
    ", 'USA ' : 'USA'\n",
    ", 'USA , AZ' : 'USA'\n",
    ", 'USA (Formerly @usNOAAgov)' : 'USA'\n",
    ", 'USA, Alabama' : 'USA'\n",
    ", 'USA, Haiti, Nepal' : 'USA'\n",
    ", 'USA, North Dakota' : 'USA'\n",
    ", 'USA, WA' : 'USA'\n",
    ", 'USA/SO FLORIDA via BROOKLYN NY' : 'USA'\n",
    ", 'USAoV' : 'USA'\n",
    ", 'Utah, USA' : 'USA'\n",
    ", 'Utica NY' : 'USA'\n",
    ", 'va' : 'USA'\n",
    ", 'Van Buren, MO' : 'USA'\n",
    ", 'Vancouver' : 'CAN'\n",
    ", 'Vancouver BC' : 'CAN'\n",
    ", 'Vancouver Canada' : 'CAN'\n",
    ", 'vancouver usa' : 'USA'\n",
    ", 'Vancouver, BC' : 'CAN'\n",
    ", 'Vancouver, BC, Canada' : 'CAN'\n",
    ", 'Vancouver, BC.' : 'CAN'\n",
    ", 'Vancouver, British Columbia' : 'CAN'\n",
    ", 'Vancouver, Canada' : 'CAN'\n",
    ", 'Vancouver, Colombie-Britannique' : 'CAN'\n",
    ", 'Ventura, Ca' : 'USA'\n",
    ", 'Vermont, USA' : 'USA'\n",
    ", 'Vero Beach , FL' : 'USA'\n",
    ", 'Very SW CA, USA....Draenor' : 'USA'\n",
    ", 'Victoria, Australia, Earth' : 'AUS'\n",
    ", 'Victoria, BC' : 'USA'\n",
    ", 'Victoria, BC  Canada' : 'CAN'\n",
    ", 'Victoria, British Columbia' : 'CAN'\n",
    ", 'Victoria, Canada' : 'CAN'\n",
    ", 'Victoria, Tx.' : 'USA'\n",
    ", 'Victorville, CA' : 'USA'\n",
    ", 'Virginia' : 'USA'\n",
    ", 'Virginia, United States' : 'USA'\n",
    ", 'Virginia, USA' : 'USA'\n",
    ", 'Vista, CA' : 'USA'\n",
    ", 'Waco TX' : 'USA'\n",
    ", 'Waco, Texas' : 'USA'\n",
    ", 'WAISTDEEP, TX' : 'USA'\n",
    ", 'Wales, United Kingdom' : 'UK'\n",
    ", 'Walker County, Alabama' : 'USA'\n",
    ", 'Walthamstow, London' : 'UK'\n",
    ", 'Wandsworth, London' : 'UK'\n",
    ", 'Warm Heart Of Africa' : 'USA'\n",
    ", 'Warrandyte, Australia' : 'AUS'\n",
    ", 'Washington' : 'USA'\n",
    ", 'Washington D.C.' : 'USA'\n",
    ", 'Washington DC' : 'USA'\n",
    ", 'Washington DC / Nantes, France' : 'USA'\n",
    ", 'Washington State' : 'USA'\n",
    ", 'washington, d.c.' : 'USA'\n",
    ", 'Washington, D.C. ' : 'USA'\n",
    ", 'Washington, D.C., area' : 'USA'\n",
    ", 'Washington, DC' : 'USA'\n",
    ", 'Washington, DC & Charlotte, NC' : 'USA'\n",
    ", 'Washington, DC 20009' : 'USA'\n",
    ", 'Washington, DC NATIVE' : 'USA'\n",
    ", 'Washington, Krasnodar (Russia)' : 'USA'\n",
    ", 'Washington, USA' : 'USA'\n",
    ", 'WASHINGTON,DC' : 'USA'\n",
    ", 'Waterford MI' : 'USA'\n",
    ", 'Waterloo, ON' : 'USA'\n",
    ", 'Waterloo, Ont' : 'USA'\n",
    ", 'Waukesha, WI' : 'USA'\n",
    ", 'Wausau, Wisconsin' : 'USA'\n",
    ", 'Waverly, IA' : 'USA'\n",
    ", 'Webster, TX' : 'USA'\n",
    ", 'West Chester, PA' : 'USA'\n",
    ", 'West Coast, Cali USA' : 'USA'\n",
    ", 'West Coast, USA' : 'USA'\n",
    ", 'West Hollywood, CA' : 'USA'\n",
    ", 'West Lancashire, UK.' : 'UK'\n",
    ", 'West Palm Beach, Florida' : 'USA'\n",
    ", 'West Richland, WA' : 'USA'\n",
    ", 'West Vancouver, B.C.' : 'CAN'\n",
    ", 'West Virginia, USA' : 'USA'\n",
    ", 'Western New York' : 'USA'\n",
    ", 'Western Washington' : 'USA'\n",
    ", 'wherever-the-fuck washington' : 'USA'\n",
    ", 'White Plains, NY' : 'USA'\n",
    ", 'Wilbraham, MA' : 'USA'\n",
    ", 'Wild Wild Web' : 'USA'\n",
    ", 'Wildomar, CA' : 'USA'\n",
    ", 'Williamsbridge, Bronx, New Yor' : 'USA'\n",
    ", 'Williamsburg, VA' : 'USA'\n",
    ", 'Williamstown, VT' : 'USA'\n",
    ", 'Wilmington, DE' : 'USA'\n",
    ", 'Wilmington, Delaware' : 'USA'\n",
    ", 'Wilmington, NC' : 'USA'\n",
    ", 'Wiltshire' : 'USA'\n",
    ", 'Windsor,Ontario' : 'CAN'\n",
    ", 'Winnipeg' : 'USA'\n",
    ", 'Winnipeg, Manitoba' : 'USA'\n",
    ", 'Winnipeg, MB, Canada' : 'CAN'\n",
    ", 'Winston Salem, North Carolina' : 'USA'\n",
    ", 'winston-salem north carolina' : 'USA'\n",
    ", 'Winston-Salem, NC' : 'USA'\n",
    ", 'Winter Park, Colorado' : 'USA'\n",
    ", 'Wisconsin' : 'USA'\n",
    ", 'Wisconsin, USA' : 'USA'\n",
    ", 'wny' : 'USA'\n",
    ", 'Wolverhampton/Brum/Jersey' : 'USA'\n",
    ", 'Wood Buffalo, Alberta' : 'CAN'\n",
    ", 'Woodcreek HS, Roseville, CA' : 'USA'\n",
    ", 'Wynne, AR' : 'USA'\n",
    ", 'Wyoming, MI (Grand Rapids)' : 'USA'\n",
    ", 'Xi\\'an, China' : 'CHN'\n",
    ", 'Yadkinville, NC' : 'USA'\n",
    ", 'Yellowknife, NT' : 'USA'\n",
    ", 'Yewa zone' : 'USA'\n",
    ", 'Ylisse' : 'USA'\n",
    ", 'Yobe State' : 'USA'\n",
    ", 'Yogya Berhati Nyaman' : 'USA'\n",
    ", 'Youngstown, OH' : 'USA'\n",
    ", 'Yuba City, CA' : 'USA'\n",
    ", 'Yulee, FL' : 'USA'\n",
    ", 'zboyer@washingtontimes.com' : 'USA'\n",
    ", 'Zeerust, South Africa' : 'ZAF'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def real_country(x):\n",
    "    if x in unify_country.keys():\n",
    "        realCountry = unify_country[x]\n",
    "    else:\n",
    "        realCountry = x\n",
    "    return realCountry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['Country'] = trainDF['location'].map( lambda text : real_country(text))\n",
    "testDF['Country'] = testDF['location'].map( lambda text : real_country(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['text'] = trainDF['text']+' '+trainDF['keyword']+' '+trainDF['location']\n",
    "testDF['text'] = testDF['text']+' '+testDF['keyword']+' '+trainDF['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pd.DataFrame(trainSparseMat.todense(), columns=countVec.get_feature_names())\n",
    "Xtest = pd.DataFrame(testSparseMat.todense(), columns=countVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstorm</th>\n",
       "      <th>access</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube playlist</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>youve</th>\n",
       "      <th>youve home</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  aba woman  abandoned  abc  abc news  ablaze  able  absolutely  \\\n",
       "0    0          0          0    0         0       0     0           0   \n",
       "1    0          0          0    0         0       0     0           0   \n",
       "2    0          0          0    0         0       0     0           0   \n",
       "3    0          0          0    0         0       0     0           0   \n",
       "4    0          0          0    0         0       0     0           0   \n",
       "\n",
       "   abstorm  access  ...  youtube  youtube playlist  youtube video  youve  \\\n",
       "0        0       0  ...        0                 0              0      0   \n",
       "1        0       0  ...        0                 0              0      0   \n",
       "2        0       0  ...        0                 0              0      0   \n",
       "3        0       0  ...        0                 0              0      0   \n",
       "4        0       0  ...        0                 0              0      0   \n",
       "\n",
       "   youve home  yr  yr old  yrs  yyc  zone  \n",
       "0           0   0       0    0    0     0  \n",
       "1           0   0       0    0    0     0  \n",
       "2           0   0       0    0    0     0  \n",
       "3           0   0       0    0    0     0  \n",
       "4           0   0       0    0    0     0  \n",
       "\n",
       "[5 rows x 2500 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainmodified = pd.concat([trainDF['Country'], Xtrain], axis=1, sort=False)\n",
    "Xtestmodified = pd.concat([testDF['Country'], Xtest], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainmodified = pd.get_dummies(Xtrainmodified,drop_first=True)\n",
    "Xtestmodified = pd.get_dummies(Xtestmodified,drop_first=True)\n",
    "common_cols = [col for col in set(Xtrainmodified.columns).intersection(Xtestmodified.columns)]\n",
    "Xtrainmodified = Xtrainmodified[common_cols]\n",
    "Xtestmodified = Xtestmodified[common_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>investigators families</th>\n",
       "      <th>heat</th>\n",
       "      <th>read</th>\n",
       "      <th>fan</th>\n",
       "      <th>making</th>\n",
       "      <th>rly</th>\n",
       "      <th>attacked</th>\n",
       "      <th>obliterated</th>\n",
       "      <th>football</th>\n",
       "      <th>outrage amid</th>\n",
       "      <th>...</th>\n",
       "      <th>derailment freakiest</th>\n",
       "      <th>fog</th>\n",
       "      <th>played</th>\n",
       "      <th>ap</th>\n",
       "      <th>sinking feeling</th>\n",
       "      <th>rising</th>\n",
       "      <th>looking</th>\n",
       "      <th>confirms</th>\n",
       "      <th>mhtwfnet</th>\n",
       "      <th>imagine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   investigators families  heat  read  fan  making  rly  attacked  \\\n",
       "0                       0     0     0    0       0    0         0   \n",
       "1                       0     0     0    0       0    0         0   \n",
       "2                       0     0     0    0       0    0         0   \n",
       "3                       0     0     0    0       0    0         0   \n",
       "4                       0     0     0    0       0    0         0   \n",
       "\n",
       "   obliterated  football  outrage amid  ...  derailment freakiest  fog  \\\n",
       "0            0         0             0  ...                     0    0   \n",
       "1            0         0             0  ...                     0    0   \n",
       "2            0         0             0  ...                     0    0   \n",
       "3            0         0             0  ...                     0    0   \n",
       "4            0         0             0  ...                     0    0   \n",
       "\n",
       "   played  ap  sinking feeling  rising  looking  confirms  mhtwfnet  imagine  \n",
       "0       0   0                0       0        0         0         0        0  \n",
       "1       0   0                0       0        0         0         0        0  \n",
       "2       0   0                0       0        0         0         0        0  \n",
       "3       0   0                0       0        0         0         0        0  \n",
       "4       0   0                0       0        0         0         0        0  \n",
       "\n",
       "[5 rows x 2522 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrainmodified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>investigators families</th>\n",
       "      <th>heat</th>\n",
       "      <th>read</th>\n",
       "      <th>fan</th>\n",
       "      <th>making</th>\n",
       "      <th>rly</th>\n",
       "      <th>attacked</th>\n",
       "      <th>obliterated</th>\n",
       "      <th>football</th>\n",
       "      <th>outrage amid</th>\n",
       "      <th>...</th>\n",
       "      <th>derailment freakiest</th>\n",
       "      <th>fog</th>\n",
       "      <th>played</th>\n",
       "      <th>ap</th>\n",
       "      <th>sinking feeling</th>\n",
       "      <th>rising</th>\n",
       "      <th>looking</th>\n",
       "      <th>confirms</th>\n",
       "      <th>mhtwfnet</th>\n",
       "      <th>imagine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   investigators families  heat  read  fan  making  rly  attacked  \\\n",
       "0                       0     0     0    0       0    0         0   \n",
       "1                       0     0     0    0       0    0         0   \n",
       "2                       0     0     0    0       0    0         0   \n",
       "3                       0     0     0    0       0    0         0   \n",
       "4                       0     0     0    0       0    0         0   \n",
       "\n",
       "   obliterated  football  outrage amid  ...  derailment freakiest  fog  \\\n",
       "0            0         0             0  ...                     0    0   \n",
       "1            0         0             0  ...                     0    0   \n",
       "2            0         0             0  ...                     0    0   \n",
       "3            0         0             0  ...                     0    0   \n",
       "4            0         0             0  ...                     0    0   \n",
       "\n",
       "   played  ap  sinking feeling  rising  looking  confirms  mhtwfnet  imagine  \n",
       "0       0   0                0       0        0         0         0        0  \n",
       "1       0   0                0       0        0         0         0        0  \n",
       "2       0   0                0       0        0         0         0        0  \n",
       "3       0   0                0       0        0         0         0        0  \n",
       "4       0   0                0       0        0         0         0        0  \n",
       "\n",
       "[5 rows x 2522 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtestmodified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "Xstrainmodified = scaler.fit_transform(Xtrainmodified)\n",
    "Xstestmodified = scaler.fit_transform(Xtestmodified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.73      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58      3263\n",
      "   macro avg       0.50      0.29      0.37      3263\n",
      "weighted avg       1.00      0.58      0.73      3263\n",
      "\n",
      "Average f1_score: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "Xtrain = Xstrainmodified\n",
    "ytrain = trainDF['target']\n",
    "Xtest = Xstestmodified\n",
    "ytest = submitDF['target']\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "y_pred = logreg.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitDF['target'] = predictions\n",
    "submitDF.to_csv('my_submission18.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this earthquake Ma...\n",
       "1                   Forest fire near La Ronge Sask Canada\n",
       "2       All residents asked to shelter in place are be...\n",
       "3        people receive wildfires evacuation orders in...\n",
       "4       Just got sent this photo from Ruby Alaska as s...\n",
       "                              ...                        \n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    ariaahrary TheTawniest The out of control wild...\n",
       "7610                        M  UTCkm S of Volcano Hawaii \n",
       "7611    Police investigating after an ebike collided w...\n",
       "7612    The Latest More Homes Razed by Northern Califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>our</th>\n",
       "      <th>reason</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>World</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country  our  reason  earthquake  this\n",
       "0      World    1       1           1     1\n",
       "1      World    0       0           0     0\n",
       "2      World    0       0           0     0\n",
       "3      World    0       0           0     0\n",
       "4      World    0       0           0     1\n",
       "...      ...  ...     ...         ...   ...\n",
       "7608   World    0       0           0     0\n",
       "7609   World    0       0           0     0\n",
       "7610   World    0       0           0     0\n",
       "7611   World    0       0           0     0\n",
       "7612   World    0       0           0     0\n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['Country','our','reason','earthquake','this']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let's see if the HashingVectorizer will enhance this or not with still the same base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.68      3263\n",
      "   macro avg       0.50      0.34      0.41      3263\n",
      "weighted avg       1.00      0.68      0.81      3263\n",
      "\n",
      "Average f1_score: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hashVec = HashingVectorizer(ngram_range=(1,2), binary=True, norm='l2')\n",
    "trainSparseMat = hashVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = hashVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "y_pred = logreg.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitDF['target'] = predictions\n",
    "submitDF.to_csv('my_submission22.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is **66%** using HashingVectorizer which is really an amazing improvement, let's see how we can improve this using different vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency - inverse document frequency (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this earthquake Ma...\n",
       "1                   Forest fire near La Ronge Sask Canada\n",
       "2       All residents asked to shelter in place are be...\n",
       "3        people receive wildfires evacuation orders in...\n",
       "4       Just got sent this photo from Ruby Alaska as s...\n",
       "                              ...                        \n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    ariaahrary TheTawniest The out of control wild...\n",
       "7610                        M  UTCkm S of Volcano Hawaii \n",
       "7611    Police investigating after an ebike collided w...\n",
       "7612    The Latest More Homes Razed by Northern Califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65      3263\n",
      "   macro avg       0.50      0.32      0.39      3263\n",
      "weighted avg       1.00      0.65      0.79      3263\n",
      "\n",
      "Average f1_score: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "termVec = TfidfVectorizer(min_df = 8, ngram_range = (1,2),binary=True ,stop_words='english') \n",
    "trainSparseMat = termVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = termVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "y_pred = logreg.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    logreg.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Resuilts as Hashing Vectorizer **65%** accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's use decission tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT CV training score:\t 0.5972685144041906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "print(\"DT CV training score:\\t\", cross_val_score(dt, Xtrain, ytrain, cv=5,n_jobs=1).mean())\n",
    "dt.fit(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.59      0.74      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.59      3263\n",
      "   macro avg       0.50      0.30      0.37      3263\n",
      "weighted avg       1.00      0.59      0.74      3263\n",
      "\n",
      "Average f1_score: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "termVec = TfidfVectorizer(stop_words='english')\n",
    "trainSparseMat = termVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = termVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(Xtrain, ytrain)\n",
    "y_pred = dt.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    dt.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = dt.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78      3263\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64      3263\n",
      "   macro avg       0.50      0.32      0.39      3263\n",
      "weighted avg       1.00      0.64      0.78      3263\n",
      "\n",
      "Average f1_score: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamer/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "termVec = TfidfVectorizer(min_df = 8, ngram_range = (1,2),stop_words='english') \n",
    "trainSparseMat = termVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = termVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "DT = DecisionTreeClassifier()\n",
    "BaggingModel = BaggingClassifier(base_estimator=DT, n_estimators=50, \n",
    "                          max_features=0.5,max_samples=0.5, oob_score=True)\n",
    "BaggingModel.fit(Xtrain, ytrain)\n",
    "y_pred = BaggingModel.predict(Xtest)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n",
    "    BaggingModel.score(Xtest, ytest)))\n",
    "##----\n",
    "predictions = BaggingModel.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying DTC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT CV training score:\t 0.6521782083394089\n",
      "DT test score:\t 0.8072326080294208\n"
     ]
    }
   ],
   "source": [
    "countVec = CountVectorizer()\n",
    "trainSparseMat = countVec.fit_transform(trainDF[\"text\"])\n",
    "testSparseMat = countVec.transform(testDF[\"text\"])\n",
    "Xtrain = trainSparseMat\n",
    "ytrain = trainDF['target']\n",
    "Xtest = testSparseMat\n",
    "ytest = submitDF['target']\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "print(\"DT CV training score:\\t\", cross_val_score(dt, Xtrain, ytrain, cv=5,\n",
    "                                                 n_jobs=1).mean())\n",
    "dt.fit(Xtrain, ytrain)\n",
    "print(\"DT test score:\\t\", dt.score(Xtest, ytest))\n",
    "\n",
    "\n",
    "# gridsearch params\n",
    "dtc_params = {\n",
    "    'max_depth': range(1,20),\n",
    "    'max_features': [None, 'log2', 'sqrt'],\n",
    "    'min_samples_split': range(5,30),\n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_samples_leaf': range(1,10)\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set the gridsearch\n",
    "dtc_gs = GridSearchCV(dt,\n",
    "                      dtc_params, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12825 candidates, totalling 64125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3632 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4256 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7222 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8960 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 11078 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14284 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 17590 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 20996 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 24502 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 28108 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 34038 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 37752 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 41684 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 47796 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 52060 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 58104 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 64125 out of 64125 | elapsed: 29.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 19, 'max_features': None, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 15}\n",
      "0.6629477231724162\n"
     ]
    }
   ],
   "source": [
    "dtc_gs.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "dtc_best = dtc_gs.best_estimator_\n",
    "print(dtc_gs.best_params_)\n",
    "print(dtc_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.83      2068\n",
      "           1       0.74      0.54      0.63      1195\n",
      "\n",
      "    accuracy                           0.76      3263\n",
      "   macro avg       0.76      0.72      0.73      3263\n",
      "weighted avg       0.76      0.76      0.75      3263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dtc_best.predict(Xtest)\n",
    "print(classification_report(ytest, predictions))\n",
    "print('Average f1_score: {:.2f}'.format(\n",
    "    f1_score(ytest, predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitDF['target'] = predictions\n",
    "submitDF.to_csv('my_submission12.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I started to load all data followed by some EDA tasked in order to see the contents of the given data and if prospect features that is useful to be used for my predictions and I'm going to describe the steps i used as follows:\n",
    "<br><br>**1. EDA** <br>\n",
    "In this phase, I had a deep look into my data to see my dataframe info and for the train dataframe I have **7613** lines whearas **3263** entries for the test dataframe. By looking into my dataframes I found null values for **keyword** and **location** fields with (**61**, **2533**),(**26**, **1105**) null entries for the trainDF, testDF respectively. My target is almost **43%** disasterious and **57%** for the rest. Also, some of the values was repeated for the location such as **USA, New York, United States**,.. etc. and for the keyword **fatalities, deluge, armageddon**,.. etc as which I decided later as i will decsibe to try to see if they are useful for my model or not(I'll explain later on). for simplicity and benefits later on i decided to replace all **NaN** values in these two fields with white space.\n",
    "<br><br>**2. Baseline Model** <br>\n",
    "in my baseline model I choose to user CountVectorizer along with logistic regression classifier with only using the text column, so i creates the sparse matrix for both the train and test dataframe and the accuracy for this model was **63%** with **0.78** for my **f1 score**\n",
    "\n",
    "<br>**3. Model Enhancements** <br><br>\n",
    "In order to enhance my model and I started to look into my text and followed the following steps to have it cleaned:<br>\n",
    "    **3.1 dealing with abbreviation issue:** here I started to look into all appreviations and i found some functions in the internet to replace these abbrevations with normal text.\n",
    "<br>**3.2 dealing with contractions issue:** here I used python module **contractions** to fix all I'm,  don't,. words inside my text and the results was pretty good to return I am , do not, and did not.\n",
    "<br>**3.3 dealing with URL's/HTML tags issue:** here i used regular expression python module to remove all URL's /HTML tagsfrom my text.\n",
    "<br>**3.4 dealing with Emojis issue:** here I used regular expression python module to remove all emojies such as :) , ;) etc.\n",
    "<br>**3.5 dealing with numbers issue:** here I used num2words python module to replace all numbers with related text but it didnt really work evectively as it replace for ex 21 to twoone which is totally different from twenty one, and despite of this issue I found a slight increase in f1 score after i used this technique but i believe i need to enhance this more and I'm pretty sure it will enhance my results dramatically.\n",
    "<br>**3.6 dealing with punctuation issue:** here I used punctuations submodule in string module remove all punctuations.\n",
    "<br>**3.7 removing stopwords:** and I have done this directly inside the vectorizer.\n",
    "<br>**3.8 lemmatizing/stemming:** I have tried bith techniques and i ended up with using only the stemming through PorterStemmer as the enhancements was obvious.\n",
    "<br><br>**4. Result** <br>\n",
    "After executing the previous tasks i manages to move my f1-score to the best results i have so far (**0.80879**).\n",
    "<img src=\"bestscore3.png\">\n",
    "<br>It is worth to mention here as well that i have tried other vectorizers like countvectorizer Hashing and tf and tried different hyperparameters and the best results was so far is CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "\n",
    "<br><br>**5. Other tasks** <br>\n",
    "I also tried the following but it that was of no significance on my model:\n",
    "5.1 adding location and keyword to the text and then follow the step# 3 again\n",
    "5.1 tried to generate a country feature from the location and use these as feature inside my model\n",
    "5.1 tried to combine similar keywords a keyword feature.\n",
    "5.1 tried to use gridsearching a decision tree classifier and see if there is an improvment\n",
    "5.1 tried to use bagging classifier with DTC as a base and see if there is an improvment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to replace all emojies with related text emotion, for ex :) to be replace to Smiling. etc. (for the record i tried this a lot but i didnt find a ready made module for this and i think i will need to write my own function to replace all emojies with related text expression)\n",
    "2. Try to fix the number to text issue to get more meanningful from the text\n",
    "3. Trying Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b <br>\n",
    "https://github.com/rishabhverma17/sms_slang_translator <br>\n",
    "https://sondosatwi.wordpress.com/2017/08/01/using-text-data-and-dataframemapper-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
